--- 
layout: single
classes: wide
title: "[LangChain] LangChain LCEL 2nd"
header:
  overlay_image: /img/langchain-bg-2.png
excerpt: 'LangChain í”„ë ˆì„ì›Œí¬ ë‚´ì—ì„œ ì²´ì¸, í”„ë¡¬í”„íŠ¸, ëª¨ë¸ì˜ ì—°ê²°ê³¼ ì¡°ì‘ì„ ë” ì‰½ê³  ê°•ë ¥í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë„êµ¬ì  ì–¸ì–´ì¸ LCEL ì— ëŒ€í•´ ì•Œì•„ë³´ì'
author: "window_for_sun"
header-style: text
categories :
  - LangChain
tags:
    - Practice
    - LangChain
    - AI
    - LLM
    - LCEL
    - Expression Language
    - configurable_fields
    - configurable_alternatives
    - RunnableWithMessageHistory
    - Runnable Graph
    - Chain Decorator
    - Custom Generator
    - Runnable Arguments Binding
toc: true
use_math: true
---  

## LangChain Expression Language



### configurable_fields
`configurable_fields` ëŠ” `Runnable` ì˜ ë‚´ë¶€ êµ¬ì„±ìš”ì†Œë¥¼ ì™¸ë¶€ì—ì„œ ë™ì ìœ¼ë¡œ êµ¬ì„±/ë³€ê²½í•  ìˆ˜ ìˆë„ë¡ ì§€ì •í•˜ëŠ” ì†ì„±ì´ë‹¤. 
ì²´ì¸ì„ ë§Œë“¤ ë•Œ, íŠ¹ì • í•„ë“œ/íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì •í•˜ì§€ ì•Šê³  ëŸ°íƒ€ì„ ì‹œì  ë˜ëŠ” ì²´ì¸ ìƒì„± ì‹œì ì— ìœ ì—°í•˜ê²Œ ê°’ì„ ì„¤ì •í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. 
ì´ë¥¼ í™œìš©í•˜ë©´ ë™ì¼í•œ `Runnable`(ì²´ì¸) êµ¬ì¡°ì— ë‹¤ì–‘í•œ ì…ë ¥/ì˜µì…˜/í™˜ê²½ì„ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤. 

`configurable_fields` ì˜ ì£¼ìš” íŠ¹ì§•ì€ ì•„ë˜ì™€ ê°™ë‹¤. 

- ë™ì  íŒŒë¼ë¯¸í„°í™” : `ì²´ì¸/Runnable` ì˜ ì¼ë¶€ ì†ì„±ì„ ëŸ°íƒ€ì„ì— ì™¸ë¶€ì—ì„œ ë³€ê²½ ê°€ëŠ¥
- ìœ ì—°í•œ ì¬ì‚¬ìš© : ë™ì¼í•œ ì²´ì¸ êµ¬ì¡°ë¥¼ ë‹¤ì–‘í•œ ìƒí™©/í™˜ê²½/ì…ë ¥ì— ë§ì¶° ì¬ì‚¬ìš© ê°€ëŠ¥
- ì½”ë“œ ê°„ì†Œí™” : ì—¬ëŸ¬ ì˜µì…˜/í™˜ê²½ì— ë§ëŠ” ì²´ì¸ í´ë˜ìŠ¤ë¥¼ ì¼ì¼ì´ ë§Œë“¤ í•„ìš” ì—†ìŒ
- ë¹ ë¥¸ ì‹¤í—˜ : íŒŒë¼ë¯¸í„° ë³€ê²½ì„ í†µí•œ ì‹¤í—˜(A/B í…ŒìŠ¤íŒ…, í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë“±)ì´ ì‰¬ì›€

ëª¨ë¸ì„ ìƒì„±í• ë•Œ `configurable_fields` ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì˜ ì¢…ë¥˜ ë° ì œê³µì²˜, í•˜ì´í¼íŒŒë¼ë¯¸í„° ë“±ì„ ëŸ°íƒ€ì„ì— ìˆ˜ì •í•  ìˆ˜ ìˆë‹¤. 

```python
from langchain.chat_models import init_chat_model
from langchain_core.runnables import ConfigurableField
from langchain_core.prompts import PromptTemplate
import os

os.environ["GROQ_API_KEY"] = "api key"
model = init_chat_model("llama-3.3-70b-versatile", model_provider="groq").configurable_fields(
    model_name=ConfigurableField(
        id="version",
        name="version of llm",
        description="offical model name"
    )
)

# ì²˜ìŒì— ì§€ì •í•œ llama-3.3-70b-versatile ëª¨ë¸ ì‚¬ìš©
model.invoke("1+1 ì€?").__dict__
# {'content': '1+1 ì€ 2ì…ë‹ˆë‹¤.',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 9,
#                                        'prompt_tokens': 40,
#                                        'total_tokens': 49,
#                                        'completion_time': 0.032727273,
#                                        'prompt_time': 0.003357174,
#                                        'queue_time': 0.205331267,
#                                        'total_time': 0.036084447},
#                        'model_name': 'llama-3.3-70b-versatile',
#                        'system_fingerprint': 'fp_3f3b593e33',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--778a623e-5d4b-431f-96d5-ceb1b220de9a-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 40,
#                     'output_tokens': 9,
#                     'total_tokens': 49}}

# ëŸ°íƒ€ì„ì— llama3-8b-8192 ëª¨ë¸ë¡œ ë³€ê²½í•´ì„œ ì‹¤í–‰
model.invoke(
    "1+1 ì€?",
    config={'configurable' : {'version': 'llama3-8b-8192'}}
).__dict__
# {'content': 'ğŸ˜Š\n\n1 + 1 = 2',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 11,
#                                        'prompt_tokens': 15,
#                                        'total_tokens': 26,
#                                        'completion_time': 0.009166667,
#                                        'prompt_time': 0.002428753,
#                                        'queue_time': 0.08527328299999999,
#                                        'total_time': 0.01159542},
#                        'model_name': 'llama3-8b-8192',
#                        'system_fingerprint': 'fp_dadc9d6142',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--4dc98968-be8d-4b13-9ea1-28c192b59e18-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 15,
#                     'output_tokens': 11,
#                     'total_tokens': 26}}

# with_config() ë¥¼ ì‚¬ìš©í•´ì„œë„ ëª¨ë¸ì„ ë³€ê²½í•  ìˆ˜ ìˆë‹¤. 
model.with_config(configurable={'version':'gemma2-9b-it'}).invoke('1+1 ì€?').__dict__
# {'content': '1 + 1ì€ 2 ì…ë‹ˆë‹¤. ğŸ˜„\n',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 13,
#                                        'prompt_tokens': 14,
#                                        'total_tokens': 27,
#                                        'completion_time': 0.024148918,
#                                        'prompt_time': 0.002076425,
#                                        'queue_time': 0.065609686,
#                                        'total_time': 0.026225343},
#                        'model_name': 'gemma2-9b-it',
#                        'system_fingerprint': 'fp_10c08bf97d',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--ed2eb76d-acce-4701-9a35-e13654f577c8-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 14,
#                     'output_tokens': 13,
#                     'total_tokens': 27}}
```  

ëª¨ë¸ ë¿ë§Œì•„ë‹ˆë¼ ì²´ì¸ì—ë„ `configurable_fields` ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.  

```python
prompt = PromptTemplate.from_template("{query} ì— ëŒ€í•´ 100ì ì´ë‚´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.")

chain = (prompt | model)

# ëª¨ë¸ ìƒì„±ì‹œ ì„¤ì •í•œ llama-3.3-70b-versatile ì‚¬ìš©
chain.invoke({"query" : "langchain"}).__dict__
# {'content': 'LangChainì€ ì–¸ì–´ ëª¨ë¸ê³¼ AIë¥¼ í™œìš©í•œ ê°œë°œ í”Œë«í¼ì…ë‹ˆë‹¤.',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 18,
#                                        'prompt_tokens': 48,
#                                        'total_tokens': 66,
#                                        'completion_time': 0.096358423,
#                                        'prompt_time': 0.002315397,
#                                        'queue_time': 0.20693972,
#                                        'total_time': 0.09867382},
#                        'model_name': 'llama-3.3-70b-versatile',
#                        'system_fingerprint': 'fp_3f3b593e33',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--760ab497-ad29-485c-bd45-8f8e04db5a52-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 48,
#                     'output_tokens': 18,
#                     'total_tokens': 66}}

# ì²´ì¸ ì‹¤í–‰ ì‹œì ì— gemma2-9b-it ëª¨ë¸ë¡œ ë³€ê²½í•´ì„œ ì‹¤í–‰
chain.with_config(configurable={'version' : 'gemma2-9b-it'}).invoke({'query':'langchain'}).__dict__
# {'content': 'LangChainì€ ëŒ€í™”í˜• AI ì•±ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. \n\ní…ìŠ¤íŠ¸ ìƒì„±, ì§ˆì˜ì‘ë‹µ, ìš”ì•½, ë²ˆì—­ ë“± ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬(NLP) ì‘ì—…ì„ ìœ„í•œ íˆ´ê³¼ ëª¨ë“ˆì„ ì œê³µí•˜ë©°, ì™¸ë¶€ ë°ì´í„°ì™€ í†µí•©í•˜ì—¬ ê°•ë ¥í•œ ì±—ë´‡ì´ë‚˜ ì¸ê³µì§€ëŠ¥ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 93,
#                                        'prompt_tokens': 24,
#                                        'total_tokens': 117,
#                                        'completion_time': 0.169090909,
#                                        'prompt_time': 0.002118285,
#                                        'queue_time': 0.08310903,
#                                        'total_time': 0.171209194},
#                        'model_name': 'gemma2-9b-it',
#                        'system_fingerprint': 'fp_10c08bf97d',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--385cc6f0-5e15-493f-86bc-07910c2277e1-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 24,
#                     'output_tokens': 93,
#                     'total_tokens': 117}}
```  

`HubRunnable` ê³¼ `configurable_fields` ë¥¼ ì‚¬ìš©í•˜ë©´ `LangChain Hub` ì— ìˆëŠ” ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¥¼ 
ìƒí™©ì— ë§ê²Œ ì‰½ê²Œ ë³€ê²½í•´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 

```python
from langchain.runnables.hub import HubRunnable

prompt = HubRunnable("hardkothari/text_summary").configurable_fields(
    owner_repo_commit=ConfigurableField(
        id="hub_commit",
        name="Hub Commit",
        description="descroption"
    )
)
prompt.invoke(
    {
        'word_count':100,
        'target_audience' :
            'children', 'text': 'ì˜¤ëŠ˜ ë°¥ì„ ë¨¹ê³  ë§›ì´ ìˆì–´ ë ˆì‹œí”¼ë¥¼ ë¸”ë¡œê·¸ì— ì‘ì„±í•´ ì˜¬ë ¸ë‹¤'
    }
)
# ChatPromptValue(messages=[SystemMessage(content='You are an expert summarizer and analyzer who can help me.', additional_kwargs={}, response_metadata={}), HumanMessage(content="Generate a concise and coherent summary from the given Context. \n\nCondense the context into a well-written summary that captures the main ideas, key points, and insights presented in the context. \n\nPrioritize clarity and brevity while retaining the essential information. \n\nAim to convey the context's core message and any supporting details that contribute to a comprehensive understanding. \n\nCraft the summary to be self-contained, ensuring that readers can grasp the content even if they haven't read the context. \n\nProvide context where necessary and avoid excessive technical jargon or verbosity.\n\nThe goal is to create a summary that effectively communicates the context's content while being easily digestible and engaging.\n\nSummary should NOT be more than 100 words for children audience.\n\nCONTEXT: ì˜¤ëŠ˜ ë°¥ì„ ë¨¹ê³  ë§›ì´ ìˆì–´ ë ˆì‹œí”¼ë¥¼ ë¸”ë¡œê·¸ì— ì‘ì„±í•´ ì˜¬ë ¸ë‹¤\n\nSUMMARY: ", additional_kwargs={}, response_metadata={})])

# with_config ë¥¼ ì‚¬ìš©í•´ í”„ë¡¬í”„íŠ¸ ë³€ê²½
prompt = prompt.with_config(
    configurable={'hub_commit' : 'langchain/summary-memory-summarizer'}
)
prompt.invoke(
    {
        'summary' : 'ì˜¤ëŠ˜ ë°¥ì„ ë¨¹ê³  ë§›ì´ ìˆì–´ ë ˆì‹œí”¼ë¥¼ ë¸”ë¡œê·¸ì— ì‘ì„±í•´ ì˜¬ë ¸ë‹¤',
        'new_lines' : '\n'
    }
)
# StringPromptValue(text='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n\nEXAMPLE\nCurrent summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n\nNew lines of conversation:\nHuman: Why do you think artificial intelligence is a force for good?\nAI: Because artificial intelligence will help humans reach their full potential.\n\nNew summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\nEND OF EXAMPLE\n\nCurrent summary:\nì˜¤ëŠ˜ ë°¥ì„ ë¨¹ê³  ë§›ì´ ìˆì–´ ë ˆì‹œí”¼ë¥¼ ë¸”ë¡œê·¸ì— ì‘ì„±í•´ ì˜¬ë ¸ë‹¤\n\nNew lines of conversation:\n\n\n\nNew summary:')
```   


### configurable_alternatives
`configurable_alternatives` ëŠ” í•˜ë‚˜ì˜ `Runnable` ë‚´ì— ì—¬ëŸ¬ ëŒ€ì¸(`Alternative`)ì„ ì„ ì–¸í•˜ê³ , 
ì‹¤í–‰ ì‹œì ì— ê·¸ ì¤‘ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í• ì§€ ì„ íƒí•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì†ì„±ì´ë‹¤. 
ë™ì‰ã„¹í•œ ì²´ì¸ êµ¬ì¡°ì—ì„œ ì—¬ëŸ¬ ëŒ€ì²´ ê°€ëŠ¥í•œ ì‹¤í–‰ ê²½ë¡œ(`LLM`, í”„ë¡¬í”„íŠ¸, íŒŒì„œ ë“±)ë¥¼ í•„ìš”ì— ë”°ë¼ ì‰½ê²Œ ë°”ê¿”ê°€ë©° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì´ë‹¤.  

`configurable_alternatives` ì˜ ì£¼ìš” íŠ¹ì§•ì€ ì•„ë˜ì™€ ê°™ë‹¤.

- ë©€í‹° ë°±ì—”ë“œ/ì˜µì…˜ ì§€ì› : í•˜ë‚˜ì˜ ì²´ì¸ì—ì„œ ì—¬ëŸ¬ `LLM`, í”„ë¡¬í”„íŠ¸, íŒŒì„œ ë“±ì„ ì‰½ê²Œ ë°”ê¿”ì¹˜ê¸°
- ì‹¤í–‰ ê²½ë¡œ ìŠ¤ìœ„ì¹­ : ìƒí™©ì— ë§ê²Œ ë‹¤ë¥¸ ëŒ€ì²´ ê²½ë¡œë¡œ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥
- ì‹¤í—˜ ë° ë¡¤ë°± : ì—¬ëŸ¬ ëŒ€ì•ˆ ì²´ì¸ì„ ì‰½ê²Œ ì‹¤í—˜í•˜ê³ , ì‹¤íŒ¨ ì‹œ ë¹ ë¥´ê²Œ ë¡¤ë°± ê°€ëŠ¥
- ì½”ë“œ ì¼ê´€ì„± : ì‹¤í–‰ ê²½ë¡œë¥¼ ì™¸ë¶€ ì„¤ì •ë§Œìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆì–´ ì½”ë“œ ìˆ˜ì • ì—†ì´ ìœ ì—°ì„± ê°•í™” 

ê°€ì¥ ë¨¼ì € ëª¨ë¸ì„ ìƒì„±í•  ë•Œ `configurable_alternatives` ë¥¼ ì‚¬ìš©í•˜ë©´ ë¯¸ë¦¬ ì •ì˜í•œ ëŒ€ì²´ì•ˆë“¤ì„ ìƒí™©ì— ë”°ë¼ ë¹ ë¥´ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.  

```python
from langchain.prompts import PromptTemplate
from langchain.chat_models import init_chat_model
from langchain_core.runnables import ConfigurableField

model = init_chat_model(
    "llama-3.3-70b-versatile", model_provider="groq"
).configurable_alternatives(
    ConfigurableField(id='llm'),
    default_key='versatile',
    gemma2=init_chat_model("gemma2-9b-it", model_provider="groq"),
    llama3=init_chat_model("llama3-8b-8192", model_provider="groq")
)

prompt = PromptTemplate.from_template('{query} ì— ëŒ€í•´ 100ì ì´ë‚´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.')

chain = (prompt | model)

# ì²´ì¸ ìƒì„± ì‹œì ì— ì§€ì •í•œ llama-3.3-70b-versatile ëª¨ë¸ ì‚¬ìš©
chain.invoke({'query':'langchain'}).__dict__
# {'content': 'LangChain: ëŒ€í™”í˜• AI í”Œë«í¼ì…ë‹ˆë‹¤.',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 14,
#                                        'prompt_tokens': 48,
#                                        'total_tokens': 62,
#                                        'completion_time': 0.077501423,
#                                        'prompt_time': 0.002599862,
#                                        'queue_time': 0.212741663,
#                                        'total_time': 0.080101285},
#                        'model_name': 'llama-3.3-70b-versatile',
#                        'system_fingerprint': 'fp_6507bcfb6f',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--b9339ba3-dd52-42b6-857b-b1ae587b4157-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 48,
#                     'output_tokens': 14,
#                     'total_tokens': 62}}

# ëŸ°íƒ€ì„ì— gemma2-9b-it ëª¨ë¸ë¡œ ë³€ê²½í•´ì„œ ì‹¤í–‰
chain.with_config(configurable={'llm':'gemma2'}).invoke({'query':'langchain'}).__dict__
# {'content': 'LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì‚¬ìš©í•˜ì—¬ ì•±ì„ êµ¬ì¶•í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. \n\nLLMì˜ ëŠ¥ë ¥ì„ í™•ì¥í•˜ê³ , ë°ì´í„°ë² ì´ìŠ¤ì™€ ê°™ì€ ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ ì—°ê²°í•˜ì—¬ ì‹¤ìš©ì ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. \n\n\n',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 75,
#                                        'prompt_tokens': 24,
#                                        'total_tokens': 99,
#                                        'completion_time': 0.136363636,
#                                        'prompt_time': 0.003373743,
#                                        'queue_time': 0.12341072400000001,
#                                        'total_time': 0.139737379},
#                        'model_name': 'gemma2-9b-it',
#                        'system_fingerprint': 'fp_10c08bf97d',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--805f5943-33d7-43f2-a45c-ddb8590eddec-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 24,
#                     'output_tokens': 75,
#                     'total_tokens': 99}}

# ëŸ°íƒ€ì„ì— llama3-8b-8192 ëª¨ë¸ë¡œ ë³€ê²½í•´ì„œ ì‹¤í–‰
chain.with_config(configurable={'llm':'llama3'}).invoke({'query':'langchain'}).__dict__
# {'content': 'LangChainì€ AI-powered language model platformì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ì„ ì§€ì›í•˜ë©°, í…ìŠ¤íŠ¸ ìƒì„±, ë¬¸ì„œ ìƒì„±, ëŒ€í™”bot ìƒì„± ë“± ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. LangChainì€ ê¸°ì¡´ì˜ NLP ê¸°ìˆ ì„ç»“åˆí•˜ì—¬, ì‚¬ìš©ìì—ê²Œ é«˜å“è³ªì˜ ì–¸ì–´ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, LangChainì€ ê°œë°©í˜• í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•˜ì—¬, ê°œë°œìë“¤ì´ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ê±°ë‚˜, ê¸°ì¡´ ê¸°ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 99,
#                                        'prompt_tokens': 23,
#                                        'total_tokens': 122,
#                                        'completion_time': 0.0825,
#                                        'prompt_time': 0.003345114,
#                                        'queue_time': 0.085356573,
#                                        'total_time': 0.085845114},
#                        'model_name': 'llama3-8b-8192',
#                        'system_fingerprint': 'fp_179b0f92c9',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--1ac4f881-8d2d-48bb-acb3-ca9b40b9af42-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 23,
#                     'output_tokens': 99,
#                     'total_tokens': 122}}
```  

`configurable_alternatives` ëŠ” í”„ë¡¬í”„íŠ¸ì—ë„ ì ìš©í•´ ë¯¸ë¦¬ ì •ì˜í•œ ëŒ€ì²´ì•ˆë“¤ì„ ìƒí™©ì— ë”°ë¼ ë¹ ë¥´ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.  

```python

prompt = PromptTemplate.from_template(
    '{query} ì— ëŒ€í•´ 100ì ì´ë‚´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.'
).configurable_alternatives(
    ConfigurableField(id='prompt'),
    default_key='anwser',
    world_count=PromptTemplate.from_template('"{query}" ì˜ ê¸€ììˆ˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.'),
    translate=PromptTemplate.from_template('"{query}" ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.')
)

chain = prompt | model

# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
chain.invoke({'query':'langchain'})
# AIMessage(content='LangChain: ì–¸ì–´ ëª¨ë¸ë§ì„ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”Œë«í¼ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 48, 'total_tokens': 69, 'completion_time': 0.103156962, 'prompt_time': 0.002634311, 'queue_time': 0.205285559, 'total_time': 0.105791273}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run--5cc88ad8-7728-4629-8fe7-52b095cc305d-0', usage_metadata={'input_tokens': 48, 'output_tokens': 21, 'total_tokens': 69})

# ê¸€ììˆ˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
chain.with_config(configurable={'prompt':'world_count'}).invoke({'query': 'langchain'})
# AIMessage(content='"langchain"ì˜ ê¸€ììˆ˜ëŠ” 9ê°œì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 46, 'total_tokens': 61, 'completion_time': 0.054545455, 'prompt_time': 0.003765855, 'queue_time': 0.24772715199999998, 'total_time': 0.05831131}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run--bf1534fe-79fc-40b4-94a1-dabea34a7899-0', usage_metadata={'input_tokens': 46, 'output_tokens': 15, 'total_tokens': 61})

# ë²ˆì—­ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
chain.with_config(configurable={'prompt':'translate'}).invoke({'query': 'ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬'})
# AIMessage(content='"ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬"ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ë©´ "Modern history of South Korea" ë˜ëŠ” "Contemporary history of South Korea"ë¡œ ë²ˆì—­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 50, 'total_tokens': 87, 'completion_time': 0.134545455, 'prompt_time': 0.002437236, 'queue_time': 0.207094754, 'total_time': 0.136982691}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run--ea17a9e9-0f28-4f66-b03a-b8fa4de492a1-0', usage_metadata={'input_tokens': 50, 'output_tokens': 37, 'total_tokens': 87})
```  

ì•ì„œ ë¨¼ì € ëª¨ë¸ì— ëŒ€í•´ `configurable_alternatives` ë¥¼ ì‚¬ìš©í•´ ëŒ€ì²´ì•ˆì„ ì„¤ì •í•˜ê³ , 
í”„ë¡¬í”„íŠ¸ì—ë„ `configurable_alternatives` ë¥¼ ì‚¬ìš©í•´ ëŒ€ì²´ì•ˆì„ ì„¤ì •í–ˆë‹¤. 
ì´ì œ ëŸ°íƒ€ì„ì— ëª¨ë¸, í”„ë¡¬í”„íŠ¸ ëª¨ë‘ í•„ìš”ì— ë”°ë¼ ëŒ€ì²´ì•ˆì„ ì„ íƒí•´ ì‹¤í–‰í•˜ ìˆ˜ ìˆë‹¤.  

```python
chain.with_config(
    configurable={
        'llm':'gemma2',
        'prompt':'world_count'
    }
).invoke({'query': 'ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬'}).__dict__
# {'content': '"ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬"ì˜ ê¸€ì ìˆ˜ëŠ” **11ê¸€ì**ì…ë‹ˆë‹¤. \n\n\n* **ëŒ€í•œë¯¼êµ­:** 6ê¸€ì\n* **ì˜:** 1ê¸€ì\n* **í˜„ëŒ€ì‚¬:** 4ê¸€ì \n',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 57,
#                                        'prompt_tokens': 27,
#                                        'total_tokens': 84,
#                                        'completion_time': 0.103636364,
#                                        'prompt_time': 0.002136625,
#                                        'queue_time': 0.083296095,
#                                        'total_time': 0.105772989},
#                        'model_name': 'gemma2-9b-it',
#                        'system_fingerprint': 'fp_10c08bf97d',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--e7dc0395-4cdc-44cc-b8df-f0cd048f7b46-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 27,
#                     'output_tokens': 57,
#                     'total_tokens': 84}}

chain.with_config(
    configurable={
        'llm':'llama3',
        'prompt':'translate'
    }
).invoke({'query': 'ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬'}).__dict__
# {'content': 'The phrase "ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬" can be translated to English as "Modern History of South Korea" or "Contemporary History of South Korea".',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 32,
#                                        'prompt_tokens': 25,
#                                        'total_tokens': 57,
#                                        'completion_time': 0.026666667,
#                                        'prompt_time': 0.009414188,
#                                        'queue_time': 1.986721293,
#                                        'total_time': 0.036080855},
#                        'model_name': 'llama3-8b-8192',
#                        'system_fingerprint': 'fp_179b0f92c9',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--3046f45b-71ce-4de1-b362-d62488914ae9-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 25,
#                     'output_tokens': 32,
#                     'total_tokens': 57}}
```  




### RunnableWithMessageHistory
`RunnableWithMessageHistory` ëŠ” `Runnable` ì— ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ê´€ë¦¬/ì£¼ì…í•˜ì—¬ ì—°ì†ì ì¸ ëŒ€í™” íë¦„ì„ ìì—°ìŠ¤ëŸ½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆê²Œ í•œë‹¤. 
í˜„ì¬ ì…ë ¥ê°’ê³¼ í•¨ê»˜ ê³¼ê±° ì£¼ê³ ë°›ì€ ë©”ì‹œì§€ë¥¼ `LLM`, í”„ë¡¬í”„íŠ¸ ë“± ë‹¤ìŒ ë‹¨ê³„ì— ìë™ìœ¼ë¡œ ë„˜ê²¨
ì—°ì†ëœ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ë©° ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•œë‹¤.  

`RunnableWithMessageHistory` ëŠ” ì•„ë˜ì™€ ê°™ì€ ê²½ìš° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

- ì±—ë´‡, ë©€í‹°í„´, ëŒ€í™”, ì»¨í…ìŠ¤íŠ¸ê°€ ì¤‘ìš”í•œ `LLM` ì›Œí¬í”Œë¡œìš°ë¥¼ ë§Œë“¤ ë•Œ 
- ëŒ€í™” ì´ë ¥ì„ ìë™ ê´€ë¦¬í•´, ë§¤ë²ˆ ì§ì ‘ ì „ë‹¬í•˜ì§€ ì•Šê³ ë„ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„ì„ ì›í•  ë•Œ 
- ì‚¬ìš©ìë³„, ì„¸ì…˜ë³„ë¡œ ë…ë¦½ì ì¸ ëŒ€í™” ìƒíƒœë¥¼ ìœ ì§€í•´ì•¼ í•  ë•Œ 
- ì—°ì†ëœ ì§ˆì˜ì‘ë‹µ/ìƒí™© ê¸°ë°˜ `LLM` íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•  ë•Œ

ë©”ì‹œì§€ ê¸°ë¡ ì €ì¥ì€ ë©”ëª¨ë¦¬, ë¡œì»¬ ì €ì¥ì†Œ, ì™¸ë¶€(`Redis`) ì €ì¥ì†Œì— ì €ì¥í•  ìˆ˜ ìˆë‹¤. 
ë¨¼ì € `BaseChatMessageHistory` ë¥¼ ì‚¬ìš©í•´ ë©”ëª¨ë¦¬ì— ì €ì¥í•´ íˆìŠ¤í† ë¦¬ë¥¼ ë³´ì¡´í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³¸ë‹¤. 
`BaseChatMessageHistory` ëŠ” ë©”ì‹œì§€ ê¸°ë¡ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ê°ì²´ë¡œ, ë©”ì‹œì§€ ê¸°ë¡ì„ ì €ì¥, ê²€ìƒ‰, ì—…ë°ì´íŠ¸í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. 
ë©”ì‹œì§€ ê¸°ë¡ì€ ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ê³  ì‚¬ìš©ìì˜ ì´ì „ ì…ë ¥ì— ê¸°ë°˜í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤.  

ë©”ëª¨ë¦¬ ë‚´ì—ì„œ ë©”ì‹œì§€ ê¸°ë¡ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•´ `ChatMessageHistory` ë¼ëŠ” `BaseChatMessageHistory` ì˜ êµ¬í˜„ì²´ë¥¼ ì‚¬ìš©í•œë‹¤.  

```python
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

model = init_chat_model(
    "llama-3.3-70b-versatile", model_provider="groq"
)
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ë‹¹ì‹ ì€ {ability} ì— ëŠ¥ìˆ™í•œ ì „ë¬¸ì‚¬ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 20ì ì´ë‚´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
        ),
        # ëŒ€í™” ê¸°ë¡ìš© ë³€ìˆ˜
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}")
    ]
)

chain = prompt | model
```  

ìœ„ ì²´ì¸ì„ ì‚¬ìš©í•´ ë©”ëª¨ë¦¬ì— ë©”ì‹œì§€ ê¸°ë¡ì„ ê´¸ë¼í•˜ë„ë¡ ì„¤ì •í•œë‹¤.  

```python
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

store = {}

def get_session_history(session_ids: str) -> BaseChatMessageHistory:
  print(f"id : {session_ids}")

  if session_ids not in store:
    store[session_ids] = ChatMessageHistory()

  return store[session_ids]

with_message_history = (
    RunnableWithMessageHistory(
        chain,
        get_session_history,
        # ì…ë ¥ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬ë  í‚¤
        input_message_key="input",
        # ì´ì „ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•  í‚¤
        history_messages_key="history"
    )
)

with_message_history.invoke(
    {"ability" : "IT", "input" : "LangChain ì— ëŒ€í•´ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ì¤˜"},
    config={'configurable':{'session_id' : 1}}
)
# id : 1
# AIMessage(content='LLaMAì™€ ê°™ì€ AI ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ê°œë°œìë“¤ì´ ë” ì‰½ê²Œ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 72, 'total_tokens': 102, 'completion_time': 0.154697818, 'prompt_time': 0.003717552, 'queue_time': 0.205502147, 'total_time': 0.15841537}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run--721c9729-53e1-4397-83da-270f15c2a60f-0', usage_metadata={'input_tokens': 72, 'output_tokens': 30, 'total_tokens': 102})

print(store)

with_message_history.invoke(
    {'ability': 'IT', 'input' : 'ì´ì „ ë‹µë³€ì„ ì˜ì–´ë¡œ ë‹µë³€í•´ì¤˜'},
    config={'configurable':{'session_id' : 1}}
)
# id : 1
# AIMessage(content='LangChain is a framework that helps developers build AI applications using models like LLaMA.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 121, 'total_tokens': 140, 'completion_time': 0.084681832, 'prompt_time': 0.008310919, 'queue_time': 0.24781782900000002, 'total_time': 0.092992751}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'finish_reason': 'stop', 'logprobs': None}, id='run--baa45c79-3800-4ca4-808c-2fba6c9940a0-0', usage_metadata={'input_tokens': 121, 'output_tokens': 19, 'total_tokens': 140})

print(store)
# {1: InMemoryChatMessageHistory(messages=[HumanMessage(content='LangChain ì— ëŒ€í•´ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ì¤˜', additional_kwargs={}, response_metadata={}), AIMessage(content='LLaMAì™€ ê°™ì€ AI ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ê°œë°œìë“¤ì´ ë” ì‰½ê²Œ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 72, 'total_tokens': 102, 'completion_time': 0.154697818, 'prompt_time': 0.003717552, 'queue_time': 0.205502147, 'total_time': 0.15841537}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run--721c9729-53e1-4397-83da-270f15c2a60f-0', usage_metadata={'input_tokens': 72, 'output_tokens': 30, 'total_tokens': 102})])}
```  

ì´ì „ ëŒ€í™” ë‚´ìš©ì„ `store` ì— ê´€ë¦¬í•˜ê¸° ë•Œë¬¸ì— ì´ì „ ë‹µë³€ ë§¥ë½ì„ ìœ ì§€í•˜ë©° ì§ˆì˜ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.   


ì•ì„  ì˜ˆì œì—ì„œëŠ” ë©”ì‹œì§€ ê¸°ë¡ì„ ì¶”ì í•˜ê³  ê´€ë¦¬í•˜ëŠ” í‚¤ë¡œ `session_id` ë¥¼ ì‚¬ìš©í–ˆë‹¤. 
ì´ëŠ” ë³„ë‹¤ë¥¸ ì„¤ì •ì„ í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ì„ ì‚¬ìš©ë˜ëŠ” í‚¤ë¡œ í•„ìš”í•˜ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì»¤ìŠ¤í…€ì´ ê°€ëŠ¥í•˜ë‹¤. 
ì•„ë˜ëŠ” `user_id` ì™€ `conversation_id` 2ê°œì˜ í‚¤ë¡œ ë©”ì‹œì§€ ê¸°ë¡ì„ ê´€ë¦¬í•˜ëŠ” ì˜ˆì œì´ë‹¤.  

```python
from langchain_core.runnables import ConfigurableFieldSpec

store_2 = {}

def get_session_history_2(user_id: str, conversation_id: str) -> BaseChatMessageHistory:
  if (user_id, conversation_id) not in store_2:
    store_2[(user_id, conversation_id)] = ChatMessageHistory()

  return store_2[(user_id, conversation_id)]

with_message_history_2 = RunnableWithMessageHistory(
    chain,
    get_session_history_2,
    input_messages_key='input',
    history_messages_key='history',
    history_factory_config=[
        ConfigurableFieldSpec(
            id='user_id',
            annotation=str,
            name="User ID",
            description='ì‚¬ìš©ì ì‹ë³„ì',
            default="",
            is_shared=True
        ),
        ConfigurableFieldSpec(
            id='conversation_id',
            annotation=str,
            name='Conversation ID',
            description='ëŒ€í™” ì‹ë³„ì',
            default='',
            is_shared=True
        )
    ]
)

with_message_history_2.invoke(
    {'ability':'IT', 'input' : '"LangChain ì— ëŒ€í•´ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ì¤˜'},
    config={
        'configurable':{
            'user_id' : 'user1',
            'conversation_id' : 'conv1'
        }
    }
)
# AIMessage(content='.LangChainì€ AIì™€ í”„ë¡œê·¸ë˜ë°ì„ ì—°ê²°í•©ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 73, 'total_tokens': 87, 'completion_time': 0.061840539, 'prompt_time': 0.005160955, 'queue_time': 0.205330767, 'total_time': 0.067001494}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run--cb5f68dd-54a2-4416-94f1-367faa6024d0-0', usage_metadata={'input_tokens': 73, 'output_tokens': 14, 'total_tokens': 87})

with_message_history_2.invoke(
    {'ability':'IT', 'input' : 'ì´ì „ ë‹µë³€ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì¤˜'},
    config={
        'configurable':{
            'user_id' : 'user1',
            'conversation_id' : 'conv1'
        }
    }
)
# AIMessage(content='LangChain connects AI and programming.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 107, 'total_tokens': 115, 'completion_time': 0.029090909, 'prompt_time': 0.005804578, 'queue_time': 0.207296262, 'total_time': 0.034895487}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'finish_reason': 'stop', 'logprobs': None}, id='run--87a73fb2-7748-44ed-a9aa-2db9724f8906-0', usage_metadata={'input_tokens': 107, 'output_tokens': 8, 'total_tokens': 115})
```  

`RunnableWithMessageHistory` ë¥¼ ì‚¬ìš©í•  ë•Œ ì…ë ¥ê³¼ ì¶œë ¥ì˜ í˜•íƒœë¥¼ í•„ìš”ì— ë”°ë¼ ì¡°ì •í•˜ë©° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 
ì•„ë˜ëŠ” `Message` ê°ì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ê²°ê³¼ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ë°›ëŠ” ì˜ˆì œì´ë‹¤.  

```python
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableParallel

chain_2 = RunnableParallel({'output_message' : model})

with_message_history_3 = RunnableWithMessageHistory(
    chain_2,
    get_session_history,
    # ì…ë ¥ìœ¼ë¡œ Message ê°ì²´ë¥¼ ë„£ê¸° ë•Œë¬¸ì— ë³„ë„ë¡œ input_message_key ë¥¼ ì§€ì •í•˜ì§€ ì•ŠëŠ”ë‹¤. 
    output_message_key='output_message'
)

with_message_history_3.invoke(
    [HumanMessage(content='langchain ì— ëŒ€í•´ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ì¤˜')],
    config={'configurable': {'session_id' : 's1'}}
)
# {'output_message': AIMessage(content='Langchainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(Large Language Model, LLM)ê³¼ ê°™ì€ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì„ ì‰½ê²Œ ì‚¬ìš©í•˜ê³  í™•ì¥í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. \n\nLangchainì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n\n1. **ì–¸ì–´ ëª¨ë¸ í†µí•©**: Langchainì€ ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ì„ ì§€ì›í•˜ì—¬ ê°œë°œìê°€ ì‰½ê²Œ ìì‹ ì˜ í”„ë¡œì íŠ¸ì— í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n2. **ì‚¬ìš©ì ì •ì˜ ê°€ëŠ¥**: Langchainì€ ê°œë°œìê°€ ìì‹ ì˜ ì–¸ì–´ ëª¨ë¸ì„ ì •ì˜í•˜ê³ , í•™ìŠµí•˜ê³ , í‰ê°€í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n3. **í™•ì¥ì„±**: Langchainì€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ê³¼ ë³µì¡í•œ ëª¨ë¸ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í™•ì¥ì„± ë†’ì€ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n4. **ì‹œê°í™” ë„êµ¬**: Langchainì€ ê°œë°œìê°€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹œê°í™”í•˜ê³ , ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\nLangchainì„ ì‚¬ìš©í•˜ë©´ ê°œë°œìëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¼ë“¤ì„ ì‰½ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n* ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì‰½ê²Œ í†µí•©í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n* ìì‹ ì˜ ì–¸ì–´ ëª¨ë¸ì„ ì •ì˜í•˜ê³ , í•™ìŠµí•˜ê³ , í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n* ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹œê°í™”í•˜ê³ , ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nLangchainì€ ìì—°ì–´ ì²˜ë¦¬, ëŒ€í™” ì‹œìŠ¤í…œ, í…ìŠ¤íŠ¸ ìƒì„± ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 288, 'prompt_tokens': 46, 'total_tokens': 334, 'completion_time': 1.047272727, 'prompt_time': 0.003877805, 'queue_time': 0.20691707399999998, 'total_time': 1.051150532}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run--6fd79132-ff13-4509-b5f2-4323cf8c2dd1-0', usage_metadata={'input_tokens': 46, 'output_tokens': 288, 'total_tokens': 334})}

with_message_history_3.invoke(
    [HumanMessage(content='ì´ì „ ë‹µë³€ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì¤˜')],
    config={'configurable': {'session_id' : 's1'}}
)
# {'output_message': AIMessage(content='Langchain is a framework that allows for easy use and extension of artificial intelligence technologies such as large language models (LLMs).\n\nLangchain has the following features:\n\n1. **Language Model Integration**: Langchain supports various language models, making it easy for developers to integrate them into their projects.\n2. **Customizability**: Langchain provides powerful tools for developers to define, train, and evaluate their own language models.\n3. **Scalability**: Langchain has a highly scalable architecture that can handle large datasets and complex models.\n4. **Visualization Tools**: Langchain provides tools for developers to visualize and analyze the performance of their models.\n\nBy using Langchain, developers can easily:\n\n* Integrate and use large language models\n* Define, train, and evaluate their own language models\n* Visualize and analyze the performance of their models\n\nLangchain can be useful in various fields such as natural language processing, conversational systems, and text generation.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 193, 'prompt_tokens': 354, 'total_tokens': 547, 'completion_time': 0.701818182, 'prompt_time': 0.052687606, 'queue_time': 0.248290259, 'total_time': 0.754505788}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run--8b7d9f42-ab8c-4248-b6f1-7028d083e49c-0', usage_metadata={'input_tokens': 354, 'output_tokens': 193, 'total_tokens': 547})}
```  

ë‹¤ìŒì€ `Message` ê°ì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , `Message` ê°ì²´ë¥¼ ì¶œë ¥ìœ¼ë¡œ ë°›ëŠ” ì˜ˆì œì´ë‹¤. 


```python
with_message_history_4 = RunnableWithMessageHistory(
    model,
    get_session_history
    # ì…ë ¥ìœ¼ë¡œ Message ê°ì²´ë¥¼ ë„£ê¸° ë•Œë¬¸ì— ë³„ë„ë¡œ input_message_key ë¥¼ ì§€ì •í•˜ì§€ ì•ŠëŠ”ë‹¤. 
    # output_message_key ë„ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ Message ê°ì²´ë¡œ ì¶œë ¥ëœë‹¤.
)

with_message_history_4.invoke(
    [HumanMessage(content='langchain ì— ëŒ€í•´ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ì¤˜')],
    config={'configurable':{'session_id':'s2'}}
)
# AIMessage(content='LangChainì€ ì¸ê³µì§€ëŠ¥ê³¼ ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì„ ê²°í•©í•˜ì—¬ ê°œë°œëœ í”Œë«í¼ì…ë‹ˆë‹¤. LangChainì€ ì‚¬ìš©ìì—ê²Œ ë”æ™ºèƒ½ì ì´ê³  ìë™í™”ëœ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. LangChainì˜ ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n1. **ì¸ê³µì§€ëŠ¥ í†µí•©**: LangChainì€ ë‹¤ì–‘í•œ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ í†µí•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë” ì •í™•í•˜ê³ æ™ºèƒ½ì ì¸ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n2. **ë¸”ë¡ì²´ì¸ ê¸°ë°˜**: LangChainì€ ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ë°ì´í„°ì˜ ë³´ì•ˆì„±ê³¼ íˆ¬ëª…ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.\n3. **ìë™í™”**: LangChainì€ ìë™í™”ëœ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.\n4. ** í™•ì¥ì„±**: LangChainì€ í™•ì¥ì„±ì´ ë›°ì–´ë‚˜ë¯€ë¡œ ì‚¬ìš©ì ìˆ˜ì˜ ì¦ê°€ì— ë”°ë¼ ì‰½ê²Œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nLangChainì€ ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ ì‘ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, LangChainì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n*æ™ºèƒ½ì ì¸ ê³ ê° ì„œë¹„ìŠ¤ ì±—ë´‡\n* ìë™í™”ëœ ë°ì´í„° ë¶„ì„ ë° ë³´ê³  ì‹œìŠ¤í…œ\n* ë³´ì•ˆì„±ì´ ë†’ì€ ë°ì´í„° ì €ì¥ ë° ê´€ë¦¬ ì‹œìŠ¤í…œ\n* ê°œì¸í™”ëœ ì¶”ì²œ ì‹œìŠ¤í…œ\n\nLangChainì€ ê°œë°œìì™€ ì‚¬ìš©ì ëª¨ë‘ì—ê²Œ í¸ë¦¬í•˜ê³  íš¨ìœ¨ì ì¸ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” í”Œë«í¼ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ LangChainì˜è¯¦ç»†í•œ ê¸°ëŠ¥ê³¼ ì‚¬ìš©ë²•ì€ ë” ì—°êµ¬í•˜ê³  í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 297, 'prompt_tokens': 46, 'total_tokens': 343, 'completion_time': 1.08, 'prompt_time': 0.003504893, 'queue_time': 0.247403755, 'total_time': 1.083504893}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'finish_reason': 'stop', 'logprobs': None}, id='run--7f524edf-b771-4536-8461-571679e345d2-0', usage_metadata={'input_tokens': 46, 'output_tokens': 297, 'total_tokens': 343})

with_message_history_4.invoke(
    [HumanMessage(content='ì´ì „ ë‹µë³€ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì¤˜')],
    config={'configurable':{'session_id':'s2'}}
)
# AIMessage(content='LangChain is a platform that combines artificial intelligence and blockchain technology. It was developed to provide users with more intelligent and automated services. The main features of LangChain are:\n\n1. **Artificial Intelligence Integration**: LangChain integrates various AI models to provide users with more accurate and intelligent services.\n2. **Blockchain-based**: LangChain is based on blockchain technology, providing security and transparency for data.\n3. **Automation**: LangChain processes user requests through automated processes, increasing efficiency.\n4. **Scalability**: LangChain is highly scalable, making it easy to expand as the number of users increases.\n\nLangChain can be applied in various fields. For example, LangChain can be used to develop the following services:\n\n* Intelligent customer service chatbots\n* Automated data analysis and reporting systems\n* Secure data storage and management systems\n* Personalized recommendation systems\n\nLangChain is a platform that provides convenient and efficient services for both developers and users. However, more research and learning are needed to understand the detailed features and usage of LangChain.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 210, 'prompt_tokens': 363, 'total_tokens': 573, 'completion_time': 0.763636364, 'prompt_time': 0.023587732, 'queue_time': 0.205756393, 'total_time': 0.787224096}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'finish_reason': 'stop', 'logprobs': None}, id='run--4a2719b4-6322-44b3-93c4-96b4e3a524bb-0', usage_metadata={'input_tokens': 363, 'output_tokens': 210, 'total_tokens': 573})
```  

ë§ˆì§€ë§‰ìœ¼ë¡œ ì…ë ¥ê³¼ ì¶œë ¥ì„ ëª¨ë‘ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œì´ë‹¤. 

```python
from operator import itemgetter

with_message_history_5 = RunnableWithMessageHistory(
    # ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ë”•ì…”ë„ˆë¦¬ì—ì„œ input_message í‚¤ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì— ì „ë‹¬í•œë‹¤. 
    itemgetter('input_message') | model,
    get_session_history,
    # ì…ë ¥ ë©”ì‹œì§€ë¡œ ì‚¬ìš©í•  í‚¤ë¥¼ ì§€ì •í•œë‹¤. 
    input_messages_key='input_message'
)

with_message_history_5.invoke(
    {'input_message' : 'langchain ì— ëŒ€í•´ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ì¤˜'},
    config={'configurable':{'session_id':'s3'}}
)
# AIMessage(content='Langchainì€ ì¸ê³µì§€ëŠ¥(AI) ê¸°ë°˜ì˜ ìì—°ì–´ ì²˜ë¦¬(NLP) í”Œë«í¼ì…ë‹ˆë‹¤. Langchainì€ ì‚¬ìš©ìì™€ ëŒ€í™”í˜•ìœ¼ë¡œ ìƒí˜¸ì‘ìš©í•˜ë©°, ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ì„œ ì´í•´í•˜ê³ , í•´ë‹¹í•˜ëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\n\nLangchainì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n1. **ìì—°ì–´ ì´í•´**: Langchainì€ ìì—°ì–´ë¥¼ ì´í•´í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì˜ë„ì™€ ìš”êµ¬ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.\n2. **ëŒ€í™”í˜• ìƒí˜¸ì‘ìš©**: Langchainì€ ì‚¬ìš©ìì™€ ëŒ€í™”í˜•ìœ¼ë¡œ ìƒí˜¸ì‘ìš©í•˜ë©°, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì— ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\n3. **ë¬¸ì„œ ìƒì„±**: Langchainì€ ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ë¬¸ì„œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n4. **ë²ˆì—­**: Langchainì€ ë‹¤ì¤‘ ì–¸ì–´ë¥¼ ì§€ì›í•˜ë©°, ì‚¬ìš©ìì˜ ì–¸ì–´ë¥¼ ìë™ìœ¼ë¡œ ë²ˆì—­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nLangchainì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ê° ì„œë¹„ìŠ¤, êµìœ¡, ì˜ë£Œ ë“±ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, Langchainì€ ê°œë°œìë“¤ì´ ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n\nLangchainì˜ ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n1. **ê³ ìœ í•œ ì•„í‚¤í…ì²˜**: Langchainì€ ê³ ìœ í•œ ì•„í‚¤í…ì²˜ë¥¼ ê°–ê³  ìˆìœ¼ë©°, ì´ëŠ” ë‹¤ë¥¸ í”Œë«í¼ê³¼ ì°¨ë³„í™”ë©ë‹ˆë‹¤.\n2. **ê³ ì„±ëŠ¥**: Langchainì€ ê³ ì„±ëŠ¥ì„ ì œê³µí•˜ë©°, ë¹ ë¥¸ ì†ë„ë¡œ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n3. **ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›**: Langchainì€ ë‹¤ì¤‘ ì–¸ì–´ë¥¼ ì§€ì›í•˜ë©°, ì‚¬ìš©ìì˜ ì–¸ì–´ë¥¼ ìë™ìœ¼ë¡œ ë²ˆì—­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nLangchainì€ ê³„ì†í•´ì„œ ë°œì „í•˜ê³  ìˆìœ¼ë©°, ìƒˆë¡œìš´ ê¸°ëŠ¥ê³¼ ì„±ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë” ì¢‹ì€ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 386, 'prompt_tokens': 46, 'total_tokens': 432, 'completion_time': 1.403636364, 'prompt_time': 0.00249485, 'queue_time': 0.20549788, 'total_time': 1.406131214}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'finish_reason': 'stop', 'logprobs': None}, id='run--bc9d121f-b849-4bd0-9aa2-d78b3fdd5f00-0', usage_metadata={'input_tokens': 46, 'output_tokens': 386, 'total_tokens': 432})

with_message_history_5.invoke(
    {'input_message' : 'ì´ì „ ë‹µë³€ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì¤˜'},
    config={'configurable':{'session_id':'s3'}}
)
# AIMessage(content="Here is the translation of the previous answer:\n\nLangchain is an artificial intelligence (AI) based natural language processing (NLP) platform. Langchain interacts with users in a conversational manner, understanding their input and providing relevant answers.\n\nThe main features of Langchain are as follows:\n\n1. **Natural Language Understanding**: Langchain understands natural language and identifies the user's intent and requirements.\n2. **Conversational Interaction**: Langchain interacts with users in a conversational manner, providing answers to their questions or requests.\n3. **Document Generation**: Langchain can generate documents based on user requests.\n4. **Translation**: Langchain supports multiple languages and can automatically translate user language.\n\nLangchain can be applied in various fields, such as customer service, education, and healthcare. Additionally, Langchain is a tool that helps developers easily develop AI-based applications.\n\nThe advantages of Langchain are as follows:\n\n1. **Unique Architecture**: Langchain has a unique architecture that differentiates it from other platforms.\n2. **High Performance**: Langchain provides high performance and can process user requests quickly.\n3. **Multi-Language Support**: Langchain supports multiple languages and can automatically translate user language.\n\nLangchain is continuously evolving, adding new features and performance to provide better services to users.", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 258, 'prompt_tokens': 452, 'total_tokens': 710, 'completion_time': 0.938181818, 'prompt_time': 0.028555753, 'queue_time': 0.206971923, 'total_time': 0.966737571}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run--c1fcd36a-adf8-4608-b26e-dc7d371caaf1-0', usage_metadata={'input_tokens': 452, 'output_tokens': 258, 'total_tokens': 710})
```  

ì´ë²ˆì—ëŠ” ë©”ëª¨ë¦¬ì— ë©”ì‹œì§€ ê¸°ë¡ì„ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ `Redis` ì™€ ê°™ì€ ì™¸ë¶€ ì €ì¥ì†Œì— ì˜ì†ì„±ì´ ìˆë„ë¡ ë©”ì‹œì§€ ê¸°ë¡ì„ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤. 
ì´ë•ŒëŠ” `BaseChatMessageHistory` ì˜ êµ¬í˜„ì²´ì¸ `RedisChatMessageHistory` ë¥¼ ì‚¬ìš©í•œë‹¤.  

```python
from langchain_community.chat_message_histories import RedisChatMessageHistory

REDIS_URL = "redis://...."

def get_redis_message_history(session_id: str) -> RedisChatMessageHistory:
  return RedisChatMessageHistory(session_id, url=REDIS_URL)

redis_with_message_history = RunnableWithMessageHistory(
    chain,
    get_redis_message_history,
    input_messages_key='input',
    history_messages_key='history'
)

redis_with_message_history.invoke(
    {'ability':'IT', 'input':'langchain ì— ëŒ€í•´ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ì¤˜'},
    config={'configurable':{'session_id':'rs1'}}
)
# AIMessage(content='LLaMA ì—ì„œ íŒŒìƒëœ ëŒ€í™”í˜• AI í”„ë ˆì„ì›Œí¬', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 72, 'total_tokens': 89, 'completion_time': 0.095742698, 'prompt_time': 0.003685556, 'queue_time': 0.248086022, 'total_time': 0.099428254}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run--bbe91fec-ee2b-494a-a9d0-43aaca41c96e-0', usage_metadata={'input_tokens': 72, 'output_tokens': 17, 'total_tokens': 89})

redis_with_message_history.invoke(
    {'ability':'IT', 'input':'ì´ì „ ë‹µë³€ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì¤˜'},
    config={'configurable':{'session_id':'rs1'}}
)
# AIMessage(content='A conversational AI framework derived from LLaMA.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 109, 'total_tokens': 121, 'completion_time': 0.050341463, 'prompt_time': 0.008292484, 'queue_time': 0.248558983, 'total_time': 0.058633947}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run--7d7c0fb9-82ba-45d7-a747-9fe90b341035-0', usage_metadata={'input_tokens': 109, 'output_tokens': 12, 'total_tokens': 121})

redis_with_message_history.invoke(
    {'ability':'IT', 'input':'ì´ì „ ë‹µë³€ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì¤˜'},
    config={'configurable':{'session_id':'rs9999999'}}
)
# AIMessage(content='There is no previous answer.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 72, 'total_tokens': 79, 'completion_time': 0.042750819, 'prompt_time': 0.004015454, 'queue_time': 0.206939182, 'total_time': 0.046766273}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run--dbc37055-edf3-4b0f-a892-5dd77aa86e46-0', usage_metadata={'input_tokens': 72, 'output_tokens': 7, 'total_tokens': 79})
```  

ì´í›„ `Redis` ì— ì ‘ì†í•´ í‚¤ë¥¼ í™•ì¸í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ `session_id` ë¡œ ì‚¬ìš©í•œ í‚¤ê°€ ìƒì„±ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.  

```bash
$ redis-cli -u redis://...

redis> keys *
1) "message_store:rs9999999"
2) "message_store:rs1"
```  


### Runnable Graph
ì•ì„œ ì‚´í´ ë³¸ ë‹¤ì–‘í•œ `LCEL` ì„ ì‚¬ìš©í•´ ì²´ì¸ì„ êµ¬ì„±í–ˆì„ ë•Œ 
ì „ì²´ì˜ ë‚´ë¶€ êµ¬ì¡°ì™€ ì‹¤í–‰ íë¦„ì„ ê·¸ë˜í”„ í˜•íƒœë¡œ ì‹œê°í™”/ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ ì²´ì¸ì˜ êµ¬ì„±(ë…¸ë“œ/ì—£ì§€ ë“±)ì„ 
ê°ì²´ë¡œ ë°˜í™˜í•˜ëŠ” ë©”ì„œë“œì´ë‹¤. 
ë³µì¡í•œ `LCEL` ì²´ì¸ì´ ì–´ë–»ê²Œ ì—°ê²°(ì¡°í•©)ë˜ì–´ ìˆëŠ”ì§€ 
ë…¸ë“œì™€ ì—£ì§€ ë‹¨ìœ„ë¡œ ê·¸ë˜í”„ ê°ì²´ë¡œ í‘œí˜„í•´ êµ¬ì¡° íŒŒì•…, ë””ë²„ê¹…, ì‹œê°í™”, ë¬¸ì„œí™” ë“±ì— í™œìš©í•  ìˆ˜ ìˆê²Œ í•œë‹¤.  

ê·¸ë˜í”„ ì˜ˆì œë¥¼ ìœ„í•´ ì•„ë˜ì™€ ê°™ì´ `LCEL` ì²´ì¸ì„ êµ¬ì„±í•œë‹¤. 
`RAG` ë¥¼ ì‚¬ìš©í•´ ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì²´ì¸ì´ë‹¤.  

```python
from langchain_chroma import Chroma
from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import init_chat_model
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough


os.environ["HUGGINGFACEHUB_API_TOKEN"] = "api key"
model_name = "BM-K/KoSimCSE-roberta"
hf_endpoint_embeddings = HuggingFaceEndpointEmbeddings(
    model=model_name,
    huggingfacehub_api_token=os.environ["HUGGINGFACEHUB_API_TOKEN"],
)

hf_embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    encode_kwargs={'normalize_embeddings':True},
)

vectorstore = Chroma.from_texts(
    [
        "ì‚¬ê³¼ëŠ” ì´ˆë¡",
        "ë°”ë‚˜ë‚˜ëŠ” ë¹¨ê°•",
        "ë”¸ê¸°ëŠ” íŒŒë‘",
        "ìˆ˜ë°•ì€ ë…¸ë‘",
        "í† ë§ˆí† ëŠ” ê²€ì •"
    ],
    embedding=hf_embeddings,
)

retriever = vectorstore.as_retriever()

template = """
ë‹¤ìŒ ë‚´ìš©ë§Œ ê³ ë ¤í•´ ì§ˆì˜ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

context: {context}

question: {question}
"""

prompt = ChatPromptTemplate.from_template(template)

chain = (
    {'context' : retriever, 'question' : RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
```  

`chain.get_graph()` ë©”ì„œë“œëŠ” ì²´ì¸ì˜ ì‹¤í–‰ ê·¸ë˜í”„ë¥¼ ë°˜í™˜í•œë‹¤. 
ê·¸ë˜í”„ì˜ ë…¸ë“œëŠ” ì²´ì¸ì˜ ê° ë‹¨ê³„ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì—£ì§€ëŠ” ê° ë‹¨ê³„ ê°„ì˜ ë°ì´í„° íë¦„ì„ ë‚˜íƒ€ë‚¸ë‹¤. 
ë¨¼ì € `chain.get_graph().nodes` ì„ í†µí•´ ì²´ì¸ì˜ ê·¸ë˜í”„ì—ì„œ ë…¸ë“œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.  

```python
chain.get_graph().nodes
# {'9a07e3cfde514a1fbe3dca8428549a2a': Node(id='9a07e3cfde514a1fbe3dca8428549a2a', name='Parallel<context,question>Input', data=<class 'langchain_core.runnables.base.RunnableParallel<context,question>Input'>, metadata=None),
# '9509fef1025d403ab6208a1c4d3138b8': Node(id='9509fef1025d403ab6208a1c4d3138b8', name='Parallel<context,question>Output', data=<class 'langchain_core.utils.pydantic.RunnableParallel<context,question>Output'>, metadata=None),
# '62bacd224938463d814b617b3cf0aeb0': Node(id='62bacd224938463d814b617b3cf0aeb0', name='VectorStoreRetriever', data=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7df1b6b30890>, search_kwargs={}), metadata=None),
# '78ab3ed409e64de7b50b16f2dfbc3258': Node(id='78ab3ed409e64de7b50b16f2dfbc3258', name='Passthrough', data=RunnablePassthrough(), metadata=None),
# 'bb09dcd82dc34c27b4136350397229f3': Node(id='bb09dcd82dc34c27b4136350397229f3', name='ChatPromptTemplate', data=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\në‹¤ìŒ ë‚´ìš©ë§Œ ê³ ë ¤í•´ ì§ˆì˜ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. \n\ncontext: {context}\n\nquestion: {question}\n'), additional_kwargs={})]), metadata=None),
# 'b5b63cec5cf34fa39c89a1aede6516dc': Node(id='b5b63cec5cf34fa39c89a1aede6516dc', name='ChatGroq', data=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7df2fb576150>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7df2fb577110>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********')), metadata=None),
# 'b39f13acf8044e3db3319cb375fe556b': Node(id='b39f13acf8044e3db3319cb375fe556b', name='StrOutputParser', data=StrOutputParser(), metadata=None),
# '93cd8025d0f44686bbd4bcd04dac927e': Node(id='93cd8025d0f44686bbd4bcd04dac927e', name='StrOutputParserOutput', data=<class 'langchain_core.output_parsers.string.StrOutputParserOutput'>, metadata=None)}
```  

`chain.get_graph().edges` ëŠ” ì²´ì¸ì˜ ê·¸ë˜í”„ì—ì„œ ì—£ì§€ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.  

```python
chain.get_graph().edges
# [Edge(source='fd4b91571d7647db989794ebe69acf4b', target='78eecced4041474ba0766aaade312ec9', data=None, conditional=False),
#  Edge(source='78eecced4041474ba0766aaade312ec9', target='28a9109205c6472682f3836f66b90c75', data=None, conditional=False),
#  Edge(source='fd4b91571d7647db989794ebe69acf4b', target='91bcfaf624f745fcaa0688bc43054461', data=None, conditional=False),
#  Edge(source='91bcfaf624f745fcaa0688bc43054461', target='28a9109205c6472682f3836f66b90c75', data=None, conditional=False),
#  Edge(source='28a9109205c6472682f3836f66b90c75', target='4925ab0ceb034ade86d275c973eff3e6', data=None, conditional=False),
#  Edge(source='4925ab0ceb034ade86d275c973eff3e6', target='db3878b3487e4474994ebd9aa80596ec', data=None, conditional=False),
#  Edge(source='aaa556d794a147e484e7e4be04f53d40', target='d488eda0ce414775866ab1707428ae3e', data=None, conditional=False),
#  Edge(source='db3878b3487e4474994ebd9aa80596ec', target='aaa556d794a147e484e7e4be04f53d40', data=None, conditional=False)]
```  

ì‹œê°í™”ëœ ê·¸ë˜í”„ í˜•íƒœë¡œ í™•ì¸í•˜ê³  ì‹¶ì€ ê²½ìš° `chain.get_graph().print_ascii()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤.  

```python
chain.get_graph().print_ascii()
#           +---------------------------------+        
#           | Parallel<context,question>Input |        
#           +---------------------------------+        
#                    ***             **                
#                  **                  ***             
#                **                       **           
# +----------------------+            +-------------+  
# | VectorStoreRetriever |            | Passthrough |  
# +----------------------+            +-------------+  
#                    ***             **                
#                       **        ***                  
#                         **    **                     
#           +----------------------------------+       
#           | Parallel<context,question>Output |       
#           +----------------------------------+       
#                             *                        
#                             *                        
#                             *                        
#                  +--------------------+              
#                  | ChatPromptTemplate |              
#                  +--------------------+              
#                             *                        
#                             *                        
#                             *                        
#                       +----------+                   
#                       | ChatGroq |                   
#                       +----------+                   
#                             *                        
#                             *                        
#                             *                        
#                   +-----------------+                
#                   | StrOutputParser |                
#                   +-----------------+                
#                             *                        
#                             *                        
#                             *                        
#                +-----------------------+             
#                | StrOutputParserOutput |             
#                +-----------------------+
```  

ë§ˆì§€ë§‰ìœ¼ë¡œ `chain.get_prompts()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ì²´ì¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” í”„ë¡¬í”„íŠ¸ì˜ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.  

```python
chain.get_prompts()
# [ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\në‹¤ìŒ ë‚´ìš©ë§Œ ê³ ë ¤í•´ ì§ˆì˜ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. \n\ncontext: {context}\n\nquestion: {question}\n'), additional_kwargs={})])]
```  


### @chain Decorator
`@chain` ë°ì½”ë ˆì´í„°ëŠ” `LCEL` ì—ì„œ ì¼ë°˜ì ì¸ íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ `LCEL` ì˜ `Runnable` ì²´ì¸ ê°ì²´ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë°ì½”ë ˆì´í„°ì´ë‹¤. 
ì´ëŠ” `RunnableLambda` ë¡œ ë˜í•‘í•˜ëŠ” ê²ƒê³¼ ê¸°ëŠ¥ì ìœ¼ë¡œ ë™ì¼í•˜ë‹¤. 
ê¸°ì¡´ì˜ í•¨ìˆ˜ë¥¼ ëª‡ ì¤„ì˜ ì½”ë“œ ìˆ˜ì • ì—†ì´ `LCEL` íŒŒì´í”„ë¼ì¸ì˜ í•œ ë‹¨ê³„ë¡œ ì‰½ê²Œ ì¡°ë¦½/í™•ì¥í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” `í•¨ìˆ˜ -> ì²´ì¸(Runnable`)` ìë™ ë³€í™˜ ì¥ì¹˜ì´ë‹¤.  

`@chain` ë°ì½”ë ˆì´í„°ëŠ” ì•„ë˜ì™€ ê°™ì€ ê²½ìš° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 

- ê¸°ì¡´ íŒŒì´ì¬ í•¨ìˆ˜/ë¡œì§ì„ `LCEL` ì²´ì¸ì— ì†ì‰½ê²Œ í†µí•©í•˜ê³  ì‹¶ì„ ë•Œ 
- í•¨ìˆ˜í˜• ì²˜ë¦¬(ì „ì²˜ë¦¬, í›„ì²˜ë¦¬, ì¡°ê±´ë¶€ ì²˜ë¦¬ ë“±)ë¥¼ ì²´ì¸ êµ¬ì„±ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì´ê³  ì‹¶ì„ ë•Œ 
- ì§ì ‘ `Runnable` í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì§€ ì•Šê³ ë„, ë¹ ë¥´ê²Œ ì²´ì¸ ìš”ì†Œë¥¼ ì¶”ê°€í•˜ê³  ì‹¶ì„ ë•Œ 
- `LCEL` ì˜ íŒŒì´í”„ë¼ì¸ ì—°ì‚°ì(`|`)ì™€ í•¨ê»˜ ì‚¬ìš©í•´, `í•¨ìˆ˜-í”„ë¡¬í”„íŠ¸-LLM-íŒŒì„œ` ë“±ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°í•©í•˜ê³  ì‹¶ì„ ë•Œ 

`@chain` í™œìš© ì˜ˆì‹œë¥¼ ë³´ì´ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ 2ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤.  

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import chain

prompt1 = ChatPromptTemplate.from_template("{query} ì— ëŒ€í•´ ì§§ê²Œ ì„¤ëª…í•˜ì„¸ìš”.")
prompt2 = ChatPromptTemplate.from_template("{setence} ë¥¼ emoji ë¥¼ ì‚¬ìš©í•´ ê¾¸ë©°ì£¼ì„¸ìš”.")
```  

ê·¸ë¦¬ê³  `@chain` ë°ì½”ë ˆì´í„°ë¡œ ì‚¬ìš©ì ì €ìœ¼ì´ í•¨ìˆ˜ë¥¼ ë°ì½”ë ˆì´íŒ… í•˜ì—¬, ì¼ë°˜ íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ `Runnable` ê°ì²´ë¡œ ë³€í™˜í•œë‹¤.  

```python
@chain
def custom_chain(text):
  chain1 = prompt1 | model | StrOutputParser()
  output1 = chain1.invoke({"query" : text})

  chain2 = prompt2 | model | StrOutputParser()

  return chain2.invoke({"setence" : output1})
```  

`custom_chain` ì€ ì•ì„œ ì •ì˜ëœ 2ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•´ì„œ ì‚¬ìš©ì ì§ˆì˜ì— ëŒ€í•œ ì„¤ëª… ë‹µë³€ì„ ìƒì„±í•˜ê³ , 
ê·¸ ë‹µë³€ì„ ë‹¤ì‹œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•´ ê¾¸ë¯¸ëŠ” ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤. 
ê·¸ë¦¬ê³  `custom_chain` ì€ ì‹¤í–‰ ê°€ëŠ¥í•œ `Runnable` ê°ì²´ì´ê¸° ë•Œë¬¸ì— `invoke()` ë¥¼ ì‚¬ìš©í•´ ì‹¤í–‰í•  ìˆ˜ ìˆë‹¤.  

```python
custom_chain.invoke('langchain')
# ğŸ¤– LangChainì€ ì¸ê³µì§€ëŠ¥ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤ ğŸ“š. LangChainì€ Pythonìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©° ğŸ, ìì—°ì–´ ì²˜ë¦¬(NLP) ğŸ“, ëŒ€í™” ì‹œìŠ¤í…œ ğŸ’¬, ê·¸ë¦¬ê³  ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ ë‹¤ì–‘í•œ ë„êµ¬ì™€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤ ğŸ‰. LangChainì„ ì‚¬ìš©í•˜ë©´ ê°œë°œìê°€ íš¨ìœ¨ì ìœ¼ë¡œ ì¸ê³µì§€ëŠ¥ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ’». ğŸš€ ê°œë°œìë“¤ì˜ ì¸ê³µì§€ëŠ¥ í™œìš©ì„ ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” LangChainì€ ì¸ê³µì§€ëŠ¥ ê°œë°œì˜æœªæ¥ë¥¼ ë°í˜€ì¤„ ê²ƒì…ë‹ˆë‹¤ ğŸ’«!
```


### Custom Generator
`Custom Generator` ëŠ” íŒŒì´ì¬ì˜ `Generator` ê¸°ëŠ¥(`yield`ë¥¼ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜)ê³¼ `LCEL` ì˜ ì²´ì¸(`Runnable`)ì‹œìŠ¤í…œì„ ê²°í•©í•˜ì—¬, 
ë°ì´í„°ë¥¼ í•œ ë²ˆì— ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ìˆœì°¨ì (ìŠ¤íŠ¸ë¦¬ë°) ìƒì„±í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ ì‹¤í–‰ ë‹¨ìœ„ë¥¼ ì˜ë¯¸í•œë‹¤. 
ì…ë ¥ê°’ì„ ë°›ì•„ ì²˜ë¦¬ ê²°ê³¼ë¥¼ `yield` ë¥¼ í†µí•´ í•œ ë‹¨ê³„ì”© ë°˜í™˜í•˜ê³  `LCEL` íŒŒì´í”„ë¼ì¸ ë‚´ì—ì„œ, 
ëŒ€ìš©ëŸ‰ ì²˜ë¦¬/ì ì§„ì  ì‘ë‹µ/ì‹¤ì‹œê°„ í”¼ë“œë°± ë“± ë‹¤ì–‘í•œ ìŠ¤íŠ¸ë¦¼ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•  ë•Œ í•µì‹¬ ì—­í• ì„ í•œë‹¤. 

`Custom Generator` ëŠ” ì•„ë˜ì™€ ê°™ì€ ê²½ìš° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

- `LLM`, `API` ë“±ì—ì„œ ì‘ë‹µì„ í•œ ë²ˆì— ëª¨ë‘ ë°›ì§€ ì•Šê³ , í† í° ë‹¨ìœ„/ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì ì§„ì  ì¶œë ¥ì´ í•„ìš”í•  ë•Œ 
- ë°ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì¼ê´„ì²˜ë¦¬ í•˜ì§€ ì•Šê³ , í•œ ì¤„ì”© ì²˜ë¦¬/ì „ì†¡í•  ë•Œ
- ìŠ¤íŠ¸ë¦¼ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ê³  ì‹¶ì„ ë•Œ 
- ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìµœì†Œí™”í•˜ë©°, ë°ì´í„° íë¦„ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì œì–´í•˜ê³  ì‹¶ì„ ë•Œ 

ë˜í•œ ì‚¬ìš©ì ì •ì˜ ì¶œë ¥ íŒŒì„œ êµ¬í˜„ ë° ì´ì „ ë‹¨ê³„ ì¶œë ¥ì„ ìˆ˜ì •í•˜ë©´ì„œ ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ì„ ìœ ì§€í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 

`Custom Generator` ì˜ˆì‹œë¥¼ ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ ì²´ì¸ì„ êµ¬í˜„í•œë‹¤.  

```python
from typing import Iterator, List
from langchain.prompts.chat import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template(
    "{query} ì— ë§ëŠ” ì£¼ìš”í•œ í‚¤ì›Œë“œ 5ê°œë¥¼ ì‰½í‘œë¡œ êµ¬ë¶„ëœ ëª©ë¡ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
)

str_chain = prompt | model | StrOutputParser()
```  

`stream()` ê³¼ `invoke()` ê²°ê³¼ë¥¼ í™•ì¸í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
for chunk in str_chain.stream({"query": "langchain"}):
    print(chunk, end="", flush=True)
# LLaMA, AI, ì–¸ì–´ ëª¨ë¸, ì¸ê³µì§€ëŠ¥, ì±—ë´‡

str_chain.invoke({"query":"langchain"})
# LLaMA, ì¸ê³µì§€ëŠ¥, ì±„íŒ…ë´‡, ì–¸ì–´ ëª¨ë¸, AI í”Œë«í¼
```  

ì•„ë˜ `split_into_list` í•¨ìˆ˜ëŠ” ì‚¬ìš©ì ì œë„ˆë ˆì´í„°ëŠ” `LLM` í† í°ì˜ `Iterator` ì…ë ¥ì•„ë¡œ ë°›ì•„ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì˜ `Iterator` ë¥¼ ë°˜í™˜í•œë‹¤.  

```python
def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:
    buffer = ""
    
    for chunk in input:
        buffer += chunk
        while "," in buffer:
            comma_index = buffer.index(",")
            yield [buffer[:comma_index].strip()]
            buffer = buffer[comma_index + 1 :]
            
    yield [buffer.strip()]
```  

`split_into_list` ì‚¬ìš©ì ì œë„ˆë ˆì´í„°ë¥¼ íŒŒì´í”„(`|`) ì—°ì‚°ìë¥¼ ì‚¬ìš©í•´ `str_chain` ì— ì—°ê²°í•œë‹¤.  

```python
list_chain = str_chain | split_into_list
```  

ì‚¬ìš©ì ì œë„ˆë ˆì´í„°ì™€ ì—°ê²°ëœ ì²´ì¸ì„ `stream()` ê³¼ `invoke()` ë¡œ ì‹¤í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ 
`LLM` ì˜ ì‘ë‹µì„ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜í•œ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.  

```python
for chunk in list_chain.stream({"query": "langchain"}):
    print(chunk, flush=True)
# ['LLaMA']
# ['ì–¸ì–´ ëª¨ë¸']
# ['ì¸ê³µì§€ëŠ¥']
# ['ì±—ë´‡']
# ['AI']

list_chain.invoke({"query" : "langchain"})
# ['Large Language Model', 'ì¸ê³µì§€ëŠ¥', 'ì±—ë´‡', 'ìì—°ì–´ ì²˜ë¦¬', 'ì–¸ì–´ ëª¨ë¸ë§']
```  

`astream()` ê³¼ `ainvoke()` ì™€ ê°™ì´ ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì‚¬ìš©ì ì œë„ˆë ˆì´í„°ë¥¼ êµ¬í˜„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.  

```python
from typing import AsyncIterator

async def asplit_into_list(input: AsyncIterator[str]) -> AsyncIterator[List[str]]:
    buffer = ""
    
    async for chunk in input:
        buffer += chunk
        while "," in buffer:
            comma_index = buffer.index(",")
            yield [buffer[:comma_index].strip()]
            buffer = buffer[comma_index + 1:]
            
    yield [buffer.strip()]

alist_chain = str_chain | asplit_into_list

async for chunk in alist_chain.astream({"query":"langchain"}):
    print(chunk, flush=True)
['AI']
['ì–¸ì–´ ëª¨ë¸']
['ì±—ë´‡']
['ìì—°ì–´ ì²˜ë¦¬']
['ê¸°ê³„ í•™ìŠµ']

await alist_chain.ainvoke({"query":"langchain"})
# ['LLaMA', 'AI', 'ì±—ë´‡', 'ìì—°ì–´ ì²˜ë¦¬', 'í”„ë¡œê·¸ë˜ë°']
```  

### Runnable Arguments Binding
`Runnable Arguents Binding` ì€ `Runnable` ì²´ì¸ ì‹¤í–‰ ì‹œì ì— ë™ì ìœ¼ë¡œ ì¸ì(íŒŒë¼ë¯¸í„°, ì˜µì…˜ ë“±)ë¥¼ ì£¼ì…í•˜ì—¬ 
ë™ì¼í•œ ì²´ì¸ êµ¬ì¡°ë¼ë„ ë§¤ë²ˆ ë‹¤ë¥´ê²Œ ë™ì‘í•˜ê²Œ ë§Œë“œëŠ” ê¸°ëŠ¥ì„ ì˜ë¯¸í•œë‹¤. 
ì²´ì¸ ì •ì˜ ì‹œ ê³ ì •ëœ ê°’ì´ ì•„ë‹ˆë¼ ì‹¤í–‰ ì‹œì ì— ì™¸ë¶€ì—ì„œ í•„ìš”í•œ ê°’ì„(í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜, ëª¨ë¸ ì˜µì…˜ ë“±)ì„ ì „ë‹¬í•´ 
ê° ë‹¨ê³„ë³„ë¡œ ì›í•˜ëŠ” ì…ë ¥, ì˜µì…˜, ë³€ìˆ˜, ì„¤ì • ë“±ì„ ìœ ì—°í•˜ê²Œ ë³€ê²½ ê°€ëŠ¥í•˜ë‹¤. 

`Runnable Arguments Binding` ì€ ì•„ë˜ì™€ ê°™ì€ ê²½ìš° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

- í”„ë¡¬í”„íŠ¸ ë‚´ ë³€ìˆ˜ ê°’, `LLM` íŒŒë¼ë¯¸í„°, ì²´ì¸ ì˜µì…˜ ë“±ì„ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ë‹¤ë¥´ê²Œ ì§€ì •í•˜ê³  ì‹¶ì€ ê²½ìš°
- ë™ì¼í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¡œ ë‹¤ì–‘í•œ ì…ë ¥ê°’/í™˜ê²½/ì˜µì…˜ì„ ì‹¤í—˜í•˜ê±°ë‚˜ ìš´ì˜í•˜ê³  ì‹¶ì„ ë•Œ
- ë™ì  ì›Œí¬í”Œë¡œìš°, ì‚¬ìš©ì ë§ì¶¤ ì‹¤í–‰, A/B í…ŒìŠ¤íŠ¸ ë“± ìƒí™©ë³„ë¡œ ì²´ì¸ ë™ì‘ì„ ë°”ê¾¸ê³  ì‹¶ì„ ë•Œ
- ì²´ì¸ ì¬ì‚¬ìš©ì„±, ìœ ì—°ì„±, í™•ì¥ì„±ì„ ê·¹ëŒ€í™”í•˜ê³  ì‹¶ì„ ë•Œ 


`Runnable Arguments Binding` ì˜ˆì‹œë¥¼ ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ ì²´ì¸ì„ êµ¬í˜„í•œë‹¤.
ìì—°ì–´ë¡œ ìˆ˜ì‹ì„ ì‘ì„±í•˜ë©´ ì´ë¥¼ ë°©ì •ì‹ì„ ë§Œë“¤ê³  ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ì´ë‹¤.  

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ëŒ€ìˆ˜ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•´ ë‹¤ìŒ ë°©ë²™ì‹ì„ ì‘ì„±í•œ ë‹¤ìŒ í’€ì´í•˜ì„¸ìš”."
            "ìµœì¢… ê²°ê³¼ëŠ” ì•„ë˜ë¥¼ ë”°ë¥´ì„¸ìš”."
            "ìˆ˜ì‹ : .."
            "ë‹µ : ..."
        ),
        (
            "human",
            "{query}"
        )
    ]
)

runnable = (
    {"query" : RunnablePassthrough()} | prompt | model | StrOutputParser()
)

runnable.invoke({"query" : "xì œê³± ë”í•˜ê¸° 7ì€ 12"})
# ìˆ˜ì‹ : x^2 + 7 = 12
# ë‹µ : x = Â±âˆš5
```  

ì—¬ê¸°ì„œ `bind()` ì˜ `stop` íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•´ ìµœì¢… ë‹µë³€ì—ì„œ íŠ¹ì • í† í°ê¹Œì§€ë§Œ ì¶œë ¥í•˜ë„ë¡ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. 
ì•„ë˜ ì˜ˆì‹œëŠ” `ìˆ˜ì‹`ê¹Œì§€ë§Œ ì¶œë ¥í•˜ê³  `ë‹µ` ì€ ì¶œë ¥í•˜ì§€ ì•Šë„ë¡ ì„¤ì •í•œ ê²ƒì´ë‹¤.  

```python
runnable_with_bind = (
    {"query" : RunnablePassthrough()}
    | prompt
    | model.bind(stop="ë‹µ")
    | StrOutputParser()
)


runnable_with_bind.invoke({"query" : "xì œê³± ë”í•˜ê¸° 7ì€ 12"})
# ìˆ˜ì‹ : x^2 + 7 = 12
```  

`binding` ì˜ ìœ ìš©í•œ í™œìš©ìœ¼ë¡œëŠ” `Functions` ê¸°ëŠ¥ì„ ì—°ê²°í•˜ëŠ” ê²ƒì´ë‹¤. 
ì•„ë˜ì™€ ê°™ì´ `ìˆ˜ì‹` ê³¼ `ë‹µ` ì„ ì¶œë ¥í•˜ëŠ” `Function` ì„ ì •ì˜í•œë‹¤.  

```python
function = {
    "name" : "solver",
    "description" : "Formulates and solves an equation",
    "parameters" : {
        "type" : "object",
        "properties" : {
            "equation" : {
                "type" : "string",
                "description" : "The algebraic expression of the equation",
            },
            "solution" : {
                "type" : "string",
                "description" : "The solution to the equation"
            }
        },
        "required" : ["equation", "solution"]
    }
}
```  

ê·¸ë¦¬ê³  `LLM` ëª¨ë¸ì— `bind()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ ì •ì˜í•œ í•¨ìˆ˜ í˜¸ì¶œì„ ëª¨ë¸ì— ë°”ì¸ë”©í•œë‹¤.  

```python
function_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ëŒ€ìˆ˜ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•´ ë‹¤ìŒ ë°©ì •ì‹ì„ ì‘ì„±í•œ ë‹¤ìŒ í’€ì´í•˜ì„¸ìš”."
        ),
        (
            "human",
            "{query}"
        )
    ]
)

function_runnable = model.bind(function_call={'name':'solver'}, functions=[function])

result = function_runnable.invoke("xì œê³± ë”í•˜ê¸° 7ì€ 12")
# AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{"equation": "x^2 + 7 = 12", "solution": "x = sqrt(5)"}', 'name': 'solver'}}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 279, 'total_tokens': 308, 'completion_time': 0.105454545, 'prompt_time': 0.024568217, 'queue_time': 0.231275051, 'total_time': 0.130022762}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'function_call', 'logprobs': None}, id='run--6bd7a989-da4d-4683-8304-38ef91f84a49-0', usage_metadata={'input_tokens': 279, 'output_tokens': 29, 'total_tokens': 308})

funciton_result = result.additional_kwargs['function_call']['arguments']
# {"equation": "x^2 + 7 = 12", "solution": "x = sqrt(5)"}
```  

ë˜ ë‹¤ë¥¸ í™œìš©ë²•ìœ¼ë¡œëŠ” `tools` ë¥¼ ì—°ê²°í•´ í™œìš©í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. 
`tools` ë¥¼ ì •ì˜í•´ ëª¨ë¸ì— ë°”ì¸ë”©í•˜ë©´ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ê°„í¸í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 
ì•„ë˜ì™€ ê°™ì€ ì§€ì—­ì˜ ë‚ ì”¨ë¥¼ í™•ì¸í•˜ëŠ” `tool` ì„ ì •ì˜í•œë‹¤.  

```python
tools = [
    {
        "type" : "function",
        "function" : {
            "name" : "get_current_weather",
            "description" : "ì£¼ì–´ì§„ ìœ„ì¹˜ì˜ í˜„ì¬ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.",
            "parameters" : {
                "type" : "object",
                "properties" : {
                    "location" : {
                        "type" : "string",
                        "description " : "ë„ì‹œì™€ ì£¼, e.g) San Francisco, CA"
                    },
                    "unit" : {
                        "type" : "string",
                        "enum" : ["celsius", "fahrenheit"]
                    },
                }
            },
            "required" : ["location"]
        }
    }
]
```  

ì •ì˜í•œ íˆ´ì„ ëª¨ë¸ì— `bind()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ ë°”ì¸ë”©í•œë‹¤. 
ê·¸ë¦¬ê³  ì—¬ëŸ¬ ì§€ì—­ì„ ì§ˆì˜ì— ë„£ì–´ ì§ˆë¬¸í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ íˆ´ì— ë§ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.  

```python
tools_model = model.bind(tools=tools)

result = tools_model.invoke("ì„œìš¸, ìƒŒí”„ë€ì‹œìŠ¤ì½”ì˜ í˜„ì¬ ë‚ ì”¨ì— ëŒ€í•´ ì•Œë ¤ì¤˜")
# AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_d75x', 'function': {'arguments': '{"location":"ì„œìš¸", "unit":"celsius"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_gw66', 'function': {'arguments': '{"location":"San Francisco, CA", "unit":"fahrenheit"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 268, 'total_tokens': 310, 'completion_time': 0.152727273, 'prompt_time': 0.023455216, 'queue_time': 0.23160565500000002, 'total_time': 0.176182489}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--753185e7-32a9-4294-9dab-2ca6782e9f6f-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'ì„œìš¸', 'unit': 'celsius'}, 'id': 'call_d75x', 'type': 'tool_call'}, {'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'fahrenheit'}, 'id': 'call_gw66', 'type': 'tool_call'}], usage_metadata={'input_tokens': 268, 'output_tokens': 42, 'total_tokens': 310})

tool_result = result.additional_kwargs['tool_calls']
# [{'id': 'call_nph9',
#   'function': {'arguments': '{"location": "ì„œìš¸", "unit": "celsius"}',
#                'name': 'get_current_weather'},
#   'type': 'function'},
#  {'id': 'call_v519',
#   'function': {'arguments': '{"location": "San Francisco, CA", "unit": "fahrenheit"}',
#                'name': 'get_current_weather'},
#   'type': 'function'}]
```  



---  
## Reference
[Understanding LangChain Runnables](https://mirascope.com/blog/langchain-runnables/)  
[runnables](https://python.langchain.com/api_reference/core/runnables.html)  
[LCEL](https://wikidocs.net/233781)  

