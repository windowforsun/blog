--- 
layout: single
classes: wide
title: "[LangChain] LangChain Prompt"
header:
  overlay_image: /img/langchain-bg-2.png
excerpt: 'LangChain ì—ì„œ Prompt ë¥¼ ì‚¬ìš©í•´ ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ì…ë ¥ì„ êµ¬ì¡°í™”í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì'
author: "window_for_sun"
header-style: text
categories :
  - LangChain
tags:
    - Practice
    - LangChain
    - AI
    - LLM
toc: true
use_math: true
---  

## LangChain Expression Language



### configurable_fields
`configurable_fields` ëŠ” `Runnable` ì˜ ë‚´ë¶€ êµ¬ì„±ìš”ì†Œë¥¼ ì™¸ë¶€ì—ì„œ ë™ì ìœ¼ë¡œ êµ¬ì„±/ë³€ê²½í•  ìˆ˜ ìˆë„ë¡ ì§€ì •í•˜ëŠ” ì†ì„±ì´ë‹¤. 
ì²´ì¸ì„ ë§Œë“¤ ë•Œ, íŠ¹ì • í•„ë“œ/íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì •í•˜ì§€ ì•Šê³  ëŸ°íƒ€ì„ ì‹œì  ë˜ëŠ” ì²´ì¸ ìƒì„± ì‹œì ì— ìœ ì—°í•˜ê²Œ ê°’ì„ ì„¤ì •í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. 
ì´ë¥¼ í™œìš©í•˜ë©´ ë™ì¼í•œ `Runnable`(ì²´ì¸) êµ¬ì¡°ì— ë‹¤ì–‘í•œ ì…ë ¥/ì˜µì…˜/í™˜ê²½ì„ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤. 

`configurable_fields` ì˜ ì£¼ìš” íŠ¹ì§•ì€ ì•„ë˜ì™€ ê°™ë‹¤. 

- ë™ì  íŒŒë¼ë¯¸í„°í™” : `ì²´ì¸/Runnable` ì˜ ì¼ë¶€ ì†ì„±ì„ ëŸ°íƒ€ì„ì— ì™¸ë¶€ì—ì„œ ë³€ê²½ ê°€ëŠ¥
- ìœ ì—°í•œ ì¬ì‚¬ìš© : ë™ì¼í•œ ì²´ì¸ êµ¬ì¡°ë¥¼ ë‹¤ì–‘í•œ ìƒí™©/í™˜ê²½/ì…ë ¥ì— ë§ì¶° ì¬ì‚¬ìš© ê°€ëŠ¥
- ì½”ë“œ ê°„ì†Œí™” : ì—¬ëŸ¬ ì˜µì…˜/í™˜ê²½ì— ë§ëŠ” ì²´ì¸ í´ë˜ìŠ¤ë¥¼ ì¼ì¼ì´ ë§Œë“¤ í•„ìš” ì—†ìŒ
- ë¹ ë¥¸ ì‹¤í—˜ : íŒŒë¼ë¯¸í„° ë³€ê²½ì„ í†µí•œ ì‹¤í—˜(A/B í…ŒìŠ¤íŒ…, í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë“±)ì´ ì‰¬ì›€

ëª¨ë¸ì„ ìƒì„±í• ë•Œ `configurable_fields` ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì˜ ì¢…ë¥˜ ë° ì œê³µì²˜, í•˜ì´í¼íŒŒë¼ë¯¸í„° ë“±ì„ ëŸ°íƒ€ì„ì— ìˆ˜ì •í•  ìˆ˜ ìˆë‹¤. 

```python
from langchain.chat_models import init_chat_model
from langchain_core.runnables import ConfigurableField
from langchain_core.prompts import PromptTemplate
import os

os.environ["GROQ_API_KEY"] = "api key"
model = init_chat_model("llama-3.3-70b-versatile", model_provider="groq").configurable_fields(
    model_name=ConfigurableField(
        id="version",
        name="version of llm",
        description="offical model name"
    )
)

# ì²˜ìŒì— ì§€ì •í•œ llama-3.3-70b-versatile ëª¨ë¸ ì‚¬ìš©
model.invoke("1+1 ì€?").__dict__
# {'content': '1+1 ì€ 2ì…ë‹ˆë‹¤.',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 9,
#                                        'prompt_tokens': 40,
#                                        'total_tokens': 49,
#                                        'completion_time': 0.032727273,
#                                        'prompt_time': 0.003357174,
#                                        'queue_time': 0.205331267,
#                                        'total_time': 0.036084447},
#                        'model_name': 'llama-3.3-70b-versatile',
#                        'system_fingerprint': 'fp_3f3b593e33',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--778a623e-5d4b-431f-96d5-ceb1b220de9a-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 40,
#                     'output_tokens': 9,
#                     'total_tokens': 49}}

# ëŸ°íƒ€ì„ì— llama3-8b-8192 ëª¨ë¸ë¡œ ë³€ê²½í•´ì„œ ì‹¤í–‰
model.invoke(
    "1+1 ì€?",
    config={'configurable' : {'version': 'llama3-8b-8192'}}
).__dict__
# {'content': 'ğŸ˜Š\n\n1 + 1 = 2',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 11,
#                                        'prompt_tokens': 15,
#                                        'total_tokens': 26,
#                                        'completion_time': 0.009166667,
#                                        'prompt_time': 0.002428753,
#                                        'queue_time': 0.08527328299999999,
#                                        'total_time': 0.01159542},
#                        'model_name': 'llama3-8b-8192',
#                        'system_fingerprint': 'fp_dadc9d6142',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--4dc98968-be8d-4b13-9ea1-28c192b59e18-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 15,
#                     'output_tokens': 11,
#                     'total_tokens': 26}}

# with_config() ë¥¼ ì‚¬ìš©í•´ì„œë„ ëª¨ë¸ì„ ë³€ê²½í•  ìˆ˜ ìˆë‹¤. 
model.with_config(configurable={'version':'gemma2-9b-it'}).invoke('1+1 ì€?').__dict__
# {'content': '1 + 1ì€ 2 ì…ë‹ˆë‹¤. ğŸ˜„\n',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 13,
#                                        'prompt_tokens': 14,
#                                        'total_tokens': 27,
#                                        'completion_time': 0.024148918,
#                                        'prompt_time': 0.002076425,
#                                        'queue_time': 0.065609686,
#                                        'total_time': 0.026225343},
#                        'model_name': 'gemma2-9b-it',
#                        'system_fingerprint': 'fp_10c08bf97d',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--ed2eb76d-acce-4701-9a35-e13654f577c8-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 14,
#                     'output_tokens': 13,
#                     'total_tokens': 27}}
```  

ëª¨ë¸ ë¿ë§Œì•„ë‹ˆë¼ ì²´ì¸ì—ë„ `configurable_fields` ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.  

```python
prompt = PromptTemplate.from_template("{query} ì— ëŒ€í•´ 100ì ì´ë‚´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.")

chain = (prompt | model)

# ëª¨ë¸ ìƒì„±ì‹œ ì„¤ì •í•œ llama-3.3-70b-versatile ì‚¬ìš©
chain.invoke({"query" : "langchain"}).__dict__
# {'content': 'LangChainì€ ì–¸ì–´ ëª¨ë¸ê³¼ AIë¥¼ í™œìš©í•œ ê°œë°œ í”Œë«í¼ì…ë‹ˆë‹¤.',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 18,
#                                        'prompt_tokens': 48,
#                                        'total_tokens': 66,
#                                        'completion_time': 0.096358423,
#                                        'prompt_time': 0.002315397,
#                                        'queue_time': 0.20693972,
#                                        'total_time': 0.09867382},
#                        'model_name': 'llama-3.3-70b-versatile',
#                        'system_fingerprint': 'fp_3f3b593e33',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--760ab497-ad29-485c-bd45-8f8e04db5a52-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 48,
#                     'output_tokens': 18,
#                     'total_tokens': 66}}

# ì²´ì¸ ì‹¤í–‰ ì‹œì ì— gemma2-9b-it ëª¨ë¸ë¡œ ë³€ê²½í•´ì„œ ì‹¤í–‰
chain.with_config(configurable={'version' : 'gemma2-9b-it'}).invoke({'query':'langchain'}).__dict__
# {'content': 'LangChainì€ ëŒ€í™”í˜• AI ì•±ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. \n\ní…ìŠ¤íŠ¸ ìƒì„±, ì§ˆì˜ì‘ë‹µ, ìš”ì•½, ë²ˆì—­ ë“± ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬(NLP) ì‘ì—…ì„ ìœ„í•œ íˆ´ê³¼ ëª¨ë“ˆì„ ì œê³µí•˜ë©°, ì™¸ë¶€ ë°ì´í„°ì™€ í†µí•©í•˜ì—¬ ê°•ë ¥í•œ ì±—ë´‡ì´ë‚˜ ì¸ê³µì§€ëŠ¥ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 93,
#                                        'prompt_tokens': 24,
#                                        'total_tokens': 117,
#                                        'completion_time': 0.169090909,
#                                        'prompt_time': 0.002118285,
#                                        'queue_time': 0.08310903,
#                                        'total_time': 0.171209194},
#                        'model_name': 'gemma2-9b-it',
#                        'system_fingerprint': 'fp_10c08bf97d',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--385cc6f0-5e15-493f-86bc-07910c2277e1-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 24,
#                     'output_tokens': 93,
#                     'total_tokens': 117}}
```  

`HubRunnable` ê³¼ `configurable_fields` ë¥¼ ì‚¬ìš©í•˜ë©´ `LangChain Hub` ì— ìˆëŠ” ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¥¼ 
ìƒí™©ì— ë§ê²Œ ì‰½ê²Œ ë³€ê²½í•´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 

```python
from langchain.runnables.hub import HubRunnable

prompt = HubRunnable("hardkothari/text_summary").configurable_fields(
    owner_repo_commit=ConfigurableField(
        id="hub_commit",
        name="Hub Commit",
        description="descroption"
    )
)
prompt.invoke(
    {
        'word_count':100,
        'target_audience' :
            'children', 'text': 'ì˜¤ëŠ˜ ë°¥ì„ ë¨¹ê³  ë§›ì´ ìˆì–´ ë ˆì‹œí”¼ë¥¼ ë¸”ë¡œê·¸ì— ì‘ì„±í•´ ì˜¬ë ¸ë‹¤'
    }
)
# ChatPromptValue(messages=[SystemMessage(content='You are an expert summarizer and analyzer who can help me.', additional_kwargs={}, response_metadata={}), HumanMessage(content="Generate a concise and coherent summary from the given Context. \n\nCondense the context into a well-written summary that captures the main ideas, key points, and insights presented in the context. \n\nPrioritize clarity and brevity while retaining the essential information. \n\nAim to convey the context's core message and any supporting details that contribute to a comprehensive understanding. \n\nCraft the summary to be self-contained, ensuring that readers can grasp the content even if they haven't read the context. \n\nProvide context where necessary and avoid excessive technical jargon or verbosity.\n\nThe goal is to create a summary that effectively communicates the context's content while being easily digestible and engaging.\n\nSummary should NOT be more than 100 words for children audience.\n\nCONTEXT: ì˜¤ëŠ˜ ë°¥ì„ ë¨¹ê³  ë§›ì´ ìˆì–´ ë ˆì‹œí”¼ë¥¼ ë¸”ë¡œê·¸ì— ì‘ì„±í•´ ì˜¬ë ¸ë‹¤\n\nSUMMARY: ", additional_kwargs={}, response_metadata={})])

# with_config ë¥¼ ì‚¬ìš©í•´ í”„ë¡¬í”„íŠ¸ ë³€ê²½
prompt = prompt.with_config(
    configurable={'hub_commit' : 'langchain/summary-memory-summarizer'}
)
prompt.invoke(
    {
        'summary' : 'ì˜¤ëŠ˜ ë°¥ì„ ë¨¹ê³  ë§›ì´ ìˆì–´ ë ˆì‹œí”¼ë¥¼ ë¸”ë¡œê·¸ì— ì‘ì„±í•´ ì˜¬ë ¸ë‹¤',
        'new_lines' : '\n'
    }
)
# StringPromptValue(text='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n\nEXAMPLE\nCurrent summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n\nNew lines of conversation:\nHuman: Why do you think artificial intelligence is a force for good?\nAI: Because artificial intelligence will help humans reach their full potential.\n\nNew summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\nEND OF EXAMPLE\n\nCurrent summary:\nì˜¤ëŠ˜ ë°¥ì„ ë¨¹ê³  ë§›ì´ ìˆì–´ ë ˆì‹œí”¼ë¥¼ ë¸”ë¡œê·¸ì— ì‘ì„±í•´ ì˜¬ë ¸ë‹¤\n\nNew lines of conversation:\n\n\n\nNew summary:')
```   


### configurable_alternatives
`configurable_alternatives` ëŠ” í•˜ë‚˜ì˜ `Runnable` ë‚´ì— ì—¬ëŸ¬ ëŒ€ì¸(`Alternative`)ì„ ì„ ì–¸í•˜ê³ , 
ì‹¤í–‰ ì‹œì ì— ê·¸ ì¤‘ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í• ì§€ ì„ íƒí•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì†ì„±ì´ë‹¤. 
ë™ì‰ã„¹í•œ ì²´ì¸ êµ¬ì¡°ì—ì„œ ì—¬ëŸ¬ ëŒ€ì²´ ê°€ëŠ¥í•œ ì‹¤í–‰ ê²½ë¡œ(`LLM`, í”„ë¡¬í”„íŠ¸, íŒŒì„œ ë“±)ë¥¼ í•„ìš”ì— ë”°ë¼ ì‰½ê²Œ ë°”ê¿”ê°€ë©° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì´ë‹¤.  

`configurable_alternatives` ì˜ ì£¼ìš” íŠ¹ì§•ì€ ì•„ë˜ì™€ ê°™ë‹¤.

- ë©€í‹° ë°±ì—”ë“œ/ì˜µì…˜ ì§€ì› : í•˜ë‚˜ì˜ ì²´ì¸ì—ì„œ ì—¬ëŸ¬ `LLM`, í”„ë¡¬í”„íŠ¸, íŒŒì„œ ë“±ì„ ì‰½ê²Œ ë°”ê¿”ì¹˜ê¸°
- ì‹¤í–‰ ê²½ë¡œ ìŠ¤ìœ„ì¹­ : ìƒí™©ì— ë§ê²Œ ë‹¤ë¥¸ ëŒ€ì²´ ê²½ë¡œë¡œ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥
- ì‹¤í—˜ ë° ë¡¤ë°± : ì—¬ëŸ¬ ëŒ€ì•ˆ ì²´ì¸ì„ ì‰½ê²Œ ì‹¤í—˜í•˜ê³ , ì‹¤íŒ¨ ì‹œ ë¹ ë¥´ê²Œ ë¡¤ë°± ê°€ëŠ¥
- ì½”ë“œ ì¼ê´€ì„± : ì‹¤í–‰ ê²½ë¡œë¥¼ ì™¸ë¶€ ì„¤ì •ë§Œìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆì–´ ì½”ë“œ ìˆ˜ì • ì—†ì´ ìœ ì—°ì„± ê°•í™” 

ê°€ì¥ ë¨¼ì € ëª¨ë¸ì„ ìƒì„±í•  ë•Œ `configurable_alternatives` ë¥¼ ì‚¬ìš©í•˜ë©´ ë¯¸ë¦¬ ì •ì˜í•œ ëŒ€ì²´ì•ˆë“¤ì„ ìƒí™©ì— ë”°ë¼ ë¹ ë¥´ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.  

```python
from langchain.prompts import PromptTemplate
from langchain.chat_models import init_chat_model
from langchain_core.runnables import ConfigurableField

model = init_chat_model(
    "llama-3.3-70b-versatile", model_provider="groq"
).configurable_alternatives(
    ConfigurableField(id='llm'),
    default_key='versatile',
    gemma2=init_chat_model("gemma2-9b-it", model_provider="groq"),
    llama3=init_chat_model("llama3-8b-8192", model_provider="groq")
)

prompt = PromptTemplate.from_template('{query} ì— ëŒ€í•´ 100ì ì´ë‚´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.')

chain = (prompt | model)

# ì²´ì¸ ìƒì„± ì‹œì ì— ì§€ì •í•œ llama-3.3-70b-versatile ëª¨ë¸ ì‚¬ìš©
chain.invoke({'query':'langchain'}).__dict__
# {'content': 'LangChain: ëŒ€í™”í˜• AI í”Œë«í¼ì…ë‹ˆë‹¤.',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 14,
#                                        'prompt_tokens': 48,
#                                        'total_tokens': 62,
#                                        'completion_time': 0.077501423,
#                                        'prompt_time': 0.002599862,
#                                        'queue_time': 0.212741663,
#                                        'total_time': 0.080101285},
#                        'model_name': 'llama-3.3-70b-versatile',
#                        'system_fingerprint': 'fp_6507bcfb6f',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--b9339ba3-dd52-42b6-857b-b1ae587b4157-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 48,
#                     'output_tokens': 14,
#                     'total_tokens': 62}}

# ëŸ°íƒ€ì„ì— gemma2-9b-it ëª¨ë¸ë¡œ ë³€ê²½í•´ì„œ ì‹¤í–‰
chain.with_config(configurable={'llm':'gemma2'}).invoke({'query':'langchain'}).__dict__
# {'content': 'LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì‚¬ìš©í•˜ì—¬ ì•±ì„ êµ¬ì¶•í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. \n\nLLMì˜ ëŠ¥ë ¥ì„ í™•ì¥í•˜ê³ , ë°ì´í„°ë² ì´ìŠ¤ì™€ ê°™ì€ ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ ì—°ê²°í•˜ì—¬ ì‹¤ìš©ì ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. \n\n\n',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 75,
#                                        'prompt_tokens': 24,
#                                        'total_tokens': 99,
#                                        'completion_time': 0.136363636,
#                                        'prompt_time': 0.003373743,
#                                        'queue_time': 0.12341072400000001,
#                                        'total_time': 0.139737379},
#                        'model_name': 'gemma2-9b-it',
#                        'system_fingerprint': 'fp_10c08bf97d',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--805f5943-33d7-43f2-a45c-ddb8590eddec-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 24,
#                     'output_tokens': 75,
#                     'total_tokens': 99}}

# ëŸ°íƒ€ì„ì— llama3-8b-8192 ëª¨ë¸ë¡œ ë³€ê²½í•´ì„œ ì‹¤í–‰
chain.with_config(configurable={'llm':'llama3'}).invoke({'query':'langchain'}).__dict__
# {'content': 'LangChainì€ AI-powered language model platformì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ì„ ì§€ì›í•˜ë©°, í…ìŠ¤íŠ¸ ìƒì„±, ë¬¸ì„œ ìƒì„±, ëŒ€í™”bot ìƒì„± ë“± ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. LangChainì€ ê¸°ì¡´ì˜ NLP ê¸°ìˆ ì„ç»“åˆí•˜ì—¬, ì‚¬ìš©ìì—ê²Œ é«˜å“è³ªì˜ ì–¸ì–´ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, LangChainì€ ê°œë°©í˜• í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•˜ì—¬, ê°œë°œìë“¤ì´ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ê±°ë‚˜, ê¸°ì¡´ ê¸°ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 99,
#                                        'prompt_tokens': 23,
#                                        'total_tokens': 122,
#                                        'completion_time': 0.0825,
#                                        'prompt_time': 0.003345114,
#                                        'queue_time': 0.085356573,
#                                        'total_time': 0.085845114},
#                        'model_name': 'llama3-8b-8192',
#                        'system_fingerprint': 'fp_179b0f92c9',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--1ac4f881-8d2d-48bb-acb3-ca9b40b9af42-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 23,
#                     'output_tokens': 99,
#                     'total_tokens': 122}}
```  

`configurable_alternatives` ëŠ” í”„ë¡¬í”„íŠ¸ì—ë„ ì ìš©í•´ ë¯¸ë¦¬ ì •ì˜í•œ ëŒ€ì²´ì•ˆë“¤ì„ ìƒí™©ì— ë”°ë¼ ë¹ ë¥´ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.  

```python

prompt = PromptTemplate.from_template(
    '{query} ì— ëŒ€í•´ 100ì ì´ë‚´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.'
).configurable_alternatives(
    ConfigurableField(id='prompt'),
    default_key='anwser',
    world_count=PromptTemplate.from_template('"{query}" ì˜ ê¸€ììˆ˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.'),
    translate=PromptTemplate.from_template('"{query}" ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.')
)

chain = prompt | model

# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
chain.invoke({'query':'langchain'})
# AIMessage(content='LangChain: ì–¸ì–´ ëª¨ë¸ë§ì„ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”Œë«í¼ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 48, 'total_tokens': 69, 'completion_time': 0.103156962, 'prompt_time': 0.002634311, 'queue_time': 0.205285559, 'total_time': 0.105791273}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run--5cc88ad8-7728-4629-8fe7-52b095cc305d-0', usage_metadata={'input_tokens': 48, 'output_tokens': 21, 'total_tokens': 69})

# ê¸€ììˆ˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
chain.with_config(configurable={'prompt':'world_count'}).invoke({'query': 'langchain'})
# AIMessage(content='"langchain"ì˜ ê¸€ììˆ˜ëŠ” 9ê°œì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 46, 'total_tokens': 61, 'completion_time': 0.054545455, 'prompt_time': 0.003765855, 'queue_time': 0.24772715199999998, 'total_time': 0.05831131}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run--bf1534fe-79fc-40b4-94a1-dabea34a7899-0', usage_metadata={'input_tokens': 46, 'output_tokens': 15, 'total_tokens': 61})

# ë²ˆì—­ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
chain.with_config(configurable={'prompt':'translate'}).invoke({'query': 'ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬'})
# AIMessage(content='"ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬"ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ë©´ "Modern history of South Korea" ë˜ëŠ” "Contemporary history of South Korea"ë¡œ ë²ˆì—­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 50, 'total_tokens': 87, 'completion_time': 0.134545455, 'prompt_time': 0.002437236, 'queue_time': 0.207094754, 'total_time': 0.136982691}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run--ea17a9e9-0f28-4f66-b03a-b8fa4de492a1-0', usage_metadata={'input_tokens': 50, 'output_tokens': 37, 'total_tokens': 87})
```  

ì•ì„œ ë¨¼ì € ëª¨ë¸ì— ëŒ€í•´ `configurable_alternatives` ë¥¼ ì‚¬ìš©í•´ ëŒ€ì²´ì•ˆì„ ì„¤ì •í•˜ê³ , 
í”„ë¡¬í”„íŠ¸ì—ë„ `configurable_alternatives` ë¥¼ ì‚¬ìš©í•´ ëŒ€ì²´ì•ˆì„ ì„¤ì •í–ˆë‹¤. 
ì´ì œ ëŸ°íƒ€ì„ì— ëª¨ë¸, í”„ë¡¬í”„íŠ¸ ëª¨ë‘ í•„ìš”ì— ë”°ë¼ ëŒ€ì²´ì•ˆì„ ì„ íƒí•´ ì‹¤í–‰í•˜ ìˆ˜ ìˆë‹¤.  

```python
chain.with_config(
    configurable={
        'llm':'gemma2',
        'prompt':'world_count'
    }
).invoke({'query': 'ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬'}).__dict__
# {'content': '"ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬"ì˜ ê¸€ì ìˆ˜ëŠ” **11ê¸€ì**ì…ë‹ˆë‹¤. \n\n\n* **ëŒ€í•œë¯¼êµ­:** 6ê¸€ì\n* **ì˜:** 1ê¸€ì\n* **í˜„ëŒ€ì‚¬:** 4ê¸€ì \n',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 57,
#                                        'prompt_tokens': 27,
#                                        'total_tokens': 84,
#                                        'completion_time': 0.103636364,
#                                        'prompt_time': 0.002136625,
#                                        'queue_time': 0.083296095,
#                                        'total_time': 0.105772989},
#                        'model_name': 'gemma2-9b-it',
#                        'system_fingerprint': 'fp_10c08bf97d',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--e7dc0395-4cdc-44cc-b8df-f0cd048f7b46-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 27,
#                     'output_tokens': 57,
#                     'total_tokens': 84}}

chain.with_config(
    configurable={
        'llm':'llama3',
        'prompt':'translate'
    }
).invoke({'query': 'ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬'}).__dict__
# {'content': 'The phrase "ëŒ€í•œë¯¼êµ­ì˜ í˜„ëŒ€ì‚¬" can be translated to English as "Modern History of South Korea" or "Contemporary History of South Korea".',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 32,
#                                        'prompt_tokens': 25,
#                                        'total_tokens': 57,
#                                        'completion_time': 0.026666667,
#                                        'prompt_time': 0.009414188,
#                                        'queue_time': 1.986721293,
#                                        'total_time': 0.036080855},
#                        'model_name': 'llama3-8b-8192',
#                        'system_fingerprint': 'fp_179b0f92c9',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--3046f45b-71ce-4de1-b362-d62488914ae9-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 25,
#                     'output_tokens': 32,
#                     'total_tokens': 57}}
```  


