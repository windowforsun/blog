--- 
layout: single
classes: wide
title: "[LangChain] LangChain Prompt"
header:
  overlay_image: /img/langchain-bg-2.png
excerpt: 'LangChain ì—ì„œ Prompt ë¥¼ ì‚¬ìš©í•´ ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ì…ë ¥ì„ êµ¬ì¡°í™”í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì'
author: "window_for_sun"
header-style: text
categories :
  - LangChain
tags:
    - Practice
    - LangChain
    - AI
    - LLM
toc: true
use_math: true
---  

## LangChain Expression Language



### configurable_fields
`configurable_fields` ëŠ” `Runnable` ì˜ ë‚´ë¶€ êµ¬ì„±ìš”ì†Œë¥¼ ì™¸ë¶€ì—ì„œ ë™ì ìœ¼ë¡œ êµ¬ì„±/ë³€ê²½í•  ìˆ˜ ìˆë„ë¡ ì§€ì •í•˜ëŠ” ì†ì„±ì´ë‹¤. 
ì²´ì¸ì„ ë§Œë“¤ ë•Œ, íŠ¹ì • í•„ë“œ/íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì •í•˜ì§€ ì•Šê³  ëŸ°íƒ€ì„ ì‹œì  ë˜ëŠ” ì²´ì¸ ìƒì„± ì‹œì ì— ìœ ì—°í•˜ê²Œ ê°’ì„ ì„¤ì •í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. 
ì´ë¥¼ í™œìš©í•˜ë©´ ë™ì¼í•œ `Runnable`(ì²´ì¸) êµ¬ì¡°ì— ë‹¤ì–‘í•œ ì…ë ¥/ì˜µì…˜/í™˜ê²½ì„ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤. 

`configurable_fields` ì˜ ì£¼ìš” íŠ¹ì§•ì€ ì•„ë˜ì™€ ê°™ë‹¤. 

- ë™ì  íŒŒë¼ë¯¸í„°í™” : `ì²´ì¸/Runnable` ì˜ ì¼ë¶€ ì†ì„±ì„ ëŸ°íƒ€ì„ì— ì™¸ë¶€ì—ì„œ ë³€ê²½ ê°€ëŠ¥
- ìœ ì—°í•œ ì¬ì‚¬ìš© : ë™ì¼í•œ ì²´ì¸ êµ¬ì¡°ë¥¼ ë‹¤ì–‘í•œ ìƒí™©/í™˜ê²½/ì…ë ¥ì— ë§ì¶° ì¬ì‚¬ìš© ê°€ëŠ¥
- ì½”ë“œ ê°„ì†Œí™” : ì—¬ëŸ¬ ì˜µì…˜/í™˜ê²½ì— ë§ëŠ” ì²´ì¸ í´ë˜ìŠ¤ë¥¼ ì¼ì¼ì´ ë§Œë“¤ í•„ìš” ì—†ìŒ
- ë¹ ë¥¸ ì‹¤í—˜ : íŒŒë¼ë¯¸í„° ë³€ê²½ì„ í†µí•œ ì‹¤í—˜(A/B í…ŒìŠ¤íŒ…, í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë“±)ì´ ì‰¬ì›€

ëª¨ë¸ì„ ìƒì„±í• ë•Œ `configurable_fields` ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì˜ ì¢…ë¥˜ ë° ì œê³µì²˜, í•˜ì´í¼íŒŒë¼ë¯¸í„° ë“±ì„ ëŸ°íƒ€ì„ì— ìˆ˜ì •í•  ìˆ˜ ìˆë‹¤. 

```python
from langchain.chat_models import init_chat_model
from langchain_core.runnables import ConfigurableField
from langchain_core.prompts import PromptTemplate
import os

os.environ["GROQ_API_KEY"] = "api key"
model = init_chat_model("llama-3.3-70b-versatile", model_provider="groq").configurable_fields(
    model_name=ConfigurableField(
        id="version",
        name="version of llm",
        description="offical model name"
    )
)

# ì²˜ìŒì— ì§€ì •í•œ llama-3.3-70b-versatile ëª¨ë¸ ì‚¬ìš©
model.invoke("1+1 ì€?").__dict__
# {'content': '1+1 ì€ 2ì…ë‹ˆë‹¤.',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 9,
#                                        'prompt_tokens': 40,
#                                        'total_tokens': 49,
#                                        'completion_time': 0.032727273,
#                                        'prompt_time': 0.003357174,
#                                        'queue_time': 0.205331267,
#                                        'total_time': 0.036084447},
#                        'model_name': 'llama-3.3-70b-versatile',
#                        'system_fingerprint': 'fp_3f3b593e33',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--778a623e-5d4b-431f-96d5-ceb1b220de9a-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 40,
#                     'output_tokens': 9,
#                     'total_tokens': 49}}

# ëŸ°íƒ€ì„ì— llama3-8b-8192 ëª¨ë¸ë¡œ ë³€ê²½í•´ì„œ ì‹¤í–‰
model.invoke(
    "1+1 ì€?",
    config={'configurable' : {'version': 'llama3-8b-8192'}}
).__dict__
# {'content': 'ğŸ˜Š\n\n1 + 1 = 2',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 11,
#                                        'prompt_tokens': 15,
#                                        'total_tokens': 26,
#                                        'completion_time': 0.009166667,
#                                        'prompt_time': 0.002428753,
#                                        'queue_time': 0.08527328299999999,
#                                        'total_time': 0.01159542},
#                        'model_name': 'llama3-8b-8192',
#                        'system_fingerprint': 'fp_dadc9d6142',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--4dc98968-be8d-4b13-9ea1-28c192b59e18-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 15,
#                     'output_tokens': 11,
#                     'total_tokens': 26}}

# with_config() ë¥¼ ì‚¬ìš©í•´ì„œë„ ëª¨ë¸ì„ ë³€ê²½í•  ìˆ˜ ìˆë‹¤. 
model.with_config(configurable={'version':'gemma2-9b-it'}).invoke('1+1 ì€?').__dict__
# {'content': '1 + 1ì€ 2 ì…ë‹ˆë‹¤. ğŸ˜„\n',
#  'additional_kwargs': {},
#  'response_metadata': {'token_usage': {'completion_tokens': 13,
#                                        'prompt_tokens': 14,
#                                        'total_tokens': 27,
#                                        'completion_time': 0.024148918,
#                                        'prompt_time': 0.002076425,
#                                        'queue_time': 0.065609686,
#                                        'total_time': 0.026225343},
#                        'model_name': 'gemma2-9b-it',
#                        'system_fingerprint': 'fp_10c08bf97d',
#                        'finish_reason': 'stop',
#                        'logprobs': None},
#  'type': 'ai',
#  'name': None,
#  'id': 'run--ed2eb76d-acce-4701-9a35-e13654f577c8-0',
#  'example': False,
#  'tool_calls': [],
#  'invalid_tool_calls': [],
#  'usage_metadata': {'input_tokens': 14,
#                     'output_tokens': 13,
#                     'total_tokens': 27}}
```  
