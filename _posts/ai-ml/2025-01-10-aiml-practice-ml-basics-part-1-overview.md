--- 
layout: single
classes: wide
title: "[AI/ML] ML Basics Part 1: Overview"
header:
  overlay_image: /img/ai-ml-bg.png
excerpt: '`Machine Learning(ML)` 의 기본 개념인 Classification 분류와 Regression 회귀의 기초 개념을 알아보자'
author: "window_for_sun"
header-style: text
categories :
  - AI/ML
tags:
    - Practice
    - ML
    - Machine Learning
    - Classification
    - Regression
    - Predictor
    - Cost Function
    - Loss Function
    - Decision Boundary
    - Normal Equation
    - Gradient Descent
    - Mean Squared Error
    - MSE
    - Linear Regression
toc: true
use_math: true
---

## ML
`Machine Learning(ML)` 은 컴퓨터가 데이터를 통해 학습하고 예측을 수행하는 기술을 의미한다. 
이런 `ML` 알고리즘을 간단하게 표현하면 `선을 긋는 것` 이라고 할 수 있다.
이후 설명에서는 이런 `선을 긋는 것` 이 어떠한 의미를 가지는 것인지 살펴본다.  

### Classification
`Classification` 은 데이터를 분류하는 작업을 의미한다. 
예를 들어, `사과` 와 `오렌지` 를 구분하는 작업을 가정해 볼 것이다. 
많은 사과와 오렌지 이미지를 모아두면, 
각 이미지에서 과일의 색상과 크기를 추출할 수 있고 이런 특징을 바탕으로 `사과` 와 `오렌지` 를 구분한다고 가정해본다. 
이때 가장 첫 단계는 `labled training data` 즉 라벨링된 학스 데이터를 얻는 것이다. 
그러므로 `사과`, `오렌지` 로 라벨링된 많은 과일 이미지를 확보하는 것이 가장 첫 단계로 필요하다. 
해당 이미지를 바탕으로 색상과 크기 정보를 추출한 뒤, 이 추출 데이터가 사과인지 오렌지인지 판별하는데 있어 어떠한 관계가 있는지 확인 할 수 있다.  

![그림 1]({{site.baseurl}}/img/aiml/aiml-ml-basics-part1-1.drawio.png)

`붉은 점` 은 사과로 라벨링된 데이터이고, `주황색 점` 은 오렌지로 라벨링된 데이터이다. 
전체 데이터를 모아 그래프에서 전체적으로 보면 라벨링된 데이터들에 특정 패턴이 있다는 것을 알 수 있다. 
사과는 대부분 빨간색이므로 그래프 좌측에 모이고, 오렌지는 주황색이기 대문에 오른쪽에 모이는 것을 볼 수 있다. 
그리고 구현하고자 하는 최종 알고리즘은 이런 패턴을 학습하도록 하는 것이다.  

알고리즘이 가장 첫 목표는 두 리벨 그룹 사이에 `decision boundary` 인 결정 경계라는 선을 그리도록 하는 것이다. 
그리고 선은 아래와 같이 그을 수 있다.  

![그림 1]({{site.baseurl}}/img/aiml/aiml-ml-basics-part1-2.drawio.png)

위와 같이 현 예제에서 라벨링된 데이터를 바탕으로 사과와 오렌지는 구분은 직선 하나로 가능하다. 
물론 더 복잡한 `ML` 알고리즘은 직선이 아닌 아래와 같은 더 복잡한 선을 그릴 수 있다.  

![그림 1]({{site.baseurl}}/img/aiml/aiml-ml-basics-part1-3.drawio.png)


라벨링된 학습 데이터를 사용했고, 이를 통해 사과와 오렌지를 구분하는 선을 그었다. 
해당 선을 바탕으로 새로운 이미지에서도 사과와 오렌지를 구분할 수 있을 것이다. 
만약 아래와 같이 `파란 점` 으로 표시된 과일 이미지를 받는다면, 우리가 학습시킨 결정 경계를 바탕으로 해당 과일 이미지는 오렌지로 분류할 수 있다.  

![그림 1]({{site.baseurl}}/img/aiml/aiml-ml-basics-part1-4.drawio.png)


이것이 바로 `ML` 을 통해 도출해 낼 수 있는 결과물이고, 
이렇게 학습 데이터를 바탕으로 `ML` 알고리즘을 수행해 데이터들 사이에 선을 그어 이를 새로운 데이터에도 적용할 수 있다. 
이렇게 데이터를 구분하기 위해 선을 긋는 `ML` 유형을 `Classification` 분류라고 한다. 
그리고 다른 하위 분야인 `Regression` 회귀는 데이터를 설명하는 선을 그리는 것과 관련이 있는데 이는 아래에서 좀 더 상세히 알아본다.  

### Regression
`Regression` 회귀는 데이터를 설명하는 선을 그리는 작업을 의미한다.
다양한 주택의 가격과 해당 주택의 면적에 관한 리벨링된 데이터가 있다고 가정한다. 
그리고 이 데이터는 아래와 같이 시각화할 수 있다.  

![그림 1]({{site.baseurl}}/img/aiml/aiml-ml-basics-part1-5.drawio.png)


각 `푸른 점` 은 주택의 면적과 가격을 나타내는 데이터를 의미하는데, 
데이터가 흩뿌려져 있는 것 같지만 동시에 특정 할 수 있는 패턴이 있다는 것도 알 수 있다. 
즉 주택이 클수록 가격이 비싸다는 경향이 있고, 이러한 패턴을 통해 주택 크기에 따라 가격을 예측할 수 있는 알고리즘을 설계할 수 있다.  

위 시각화된 데이터가 대각선 방향으로 흩뿌려져 있는 것을 볼 수 있는데, 
이를 일반화해 주택이 대각선 방향의 데이터 군집에 위치할 가능성이 높다고 판단할 수 있다. 
아래 그림에서 새로운 주택 데이터가 `초록 점` 에 위치할 가능성은 매우 높고, 
`빨간 점` 에 위치할 가능성은 매우 낮다고 할 수 있다.  

![그림 1]({{site.baseurl}}/img/aiml/aiml-ml-basics-part1-6.drawio.png)


이러한 패턴을 좀 더 일반화하면, 주어진 면적에 대해 주택 가격이 얼마인지 정확한 답은 구하기 어렵지만, 
대략적인 답은 구하기 쉽다. 
이런 일반화를 위해 흩뿌려져 있는 데이터 군집을 가로지르는 각 데이터에 가장 가까운 선을 그린다. 
이러한 선을 `predicator` 예측기라고 하고, 이를 바탕으로 주택 면적을 기반으로 주택 가격을 예측할 수 있다. 
해당 대각선을 통해 주어진 주택 면적에 대한 가능성이 높은 주택가격을 도출할 수 있다. 
이를 다른 의미로 표현하면 해당 예측기는 주어진 면적에 대한 주택 가격의 `평균` 을 예측한다고 할 수 있다.  

![그림 1]({{site.baseurl}}/img/aiml/aiml-ml-basics-part1-7.drawio.png)


예측기인 선이 반드시 선형(`linear`) 이어야 하는 것은 아니다. 
예측기는 알고 있는 다양하고 좀 더 복잡한 함수 나 모델(이차 함수, 사인 함수, 임의의 함수 등) 일 수 있다. 
예측기에 복잡한 모델을 사용한다고 해서 예측값의 정확도가 올라간다고 할 수 있는 것이 아니기 때문에, 
어떤 모델을 사용할지는 문제에 따라 데이터에 따라 달라 질 수 있음을 기억해야 한다.  

현재 구현된 예측기는 하나의 입력 변수(주택 면적)를 가지고 있지만, 
위치, 건축 자제 등 더 많은 정보를 함께 고려할 수도 있다. 
그러면 그래프는 3차원이상으로 확장되고, 예측기는 면적과 위치, 건축 자재 등을 고려한 복잡한 모델이 될 것이다.  

만약 그래프가 3차원이라도 데이터를 기반으로 예측기를 그릴 수 있는데, 이때는 선이 아니라 편면을 그려야 할 것이다. 
즉 주택 가격을 예측하기 위해 면적과 위치, 건축 자재 등을 고려한 예측기를 그려야 하기 때문이다.  

실제로 더 복잡한 `ML` 학습 알고리즘에서는 수백 개에서 수천 개의 변수를 사용해 결과를 고려할 수 있다.  

### Predictor
`Predictor` 예측기는 데이터를 바탕으로 예측을 수행하는 모델을 의미한다. 
앞선 언급처럼 예측기에는 다양한 종류가 있고, 위 글에서 사용한 것은 선형 예측기이다.
이런 선형 예측기를 수학적 형태로 표현하면 아래와 같다.


$$
f(x) = c_nx_n + c_{n-1}x_{n-1} + ... + c_1x_1 + c_0x_0
$$


위 식에서 `x` 는 면적이나 생활비 같은 서로 다른 입력 특성을 나타내고, 
`c` 는 파라미터, 가중치 라고 부른다. 
특정 가중치가 클수록 모델이 해당 특성을 더 중요하게 고려한다는 의미이다. 
주택 면적의 경우 주택 가격 예측에 좋은 변수 이므로, 
알고리즘에서 해당 변수의 가중치를 높여 해당 변수를 더 중요하게 고려할 수 있다. 
그리고 주택 콘센트의 수와 같은 변수는 주택 가격 예측에 중요하지 않은 변수이므로 해당 변수의 가중치를 낮출 수 있다.  

결과적으로 주택 크기에 따른 주택 가격을 예측하는 경우에는 면적이라는 하나의 변수만 고려하기 때문에, 
입력 `x` 는 하나만 필요하므로 아래와 같이 정의할 수 있다.  


$$
y(x) = c_1x + c_0
$$

그리고 이는 아래와 같이 좀 더 일반화해 표현할 수도 있다.  


$$
y(x) = mx + b
$$


`y(x)` 는 출력 값 즉 주택 가격이고, `x` 는 입력 특성인 주택 크기, 
$c_0$ 는 `y` 축과 만나는 지점으로 `x` 가 0일 때 `y` 의 값을 의미한다. 
즉 이는 `y` 절편(`intercept`) 이라고 라고 불리며, 주택의 기본 가격을 의미한다.
이런 $c_0$ 를 사용해 예측모델 전반의 편항(`bias`) 을 조정할 수 있다.  


여기서 중요한 점은 `ML` 알고리즘에서 예측 결과를 잘 도출할 수 있는 가중치/파라미터($c_1$, $c_0$) 를 어떻게 찾을 지에 대한 고민이 남는다. 
먼저 `정규 방정식`(`normal equation`) 이라는 방법을 사용해 가중치를 찾을 수 있지만, 
이는 행렬을 통해 계수를 작접 계산하는 방법으로 한 번의 계산으로 정확한 해를 구할 수는 있지만, 
변수가 많이질 수록 계산 비용이 급격히 증가한다는 점이있다.  
그래서 일반적으로 이후에 더 자세히 알아볼 `Gradient Descent` 방법을 사용해 가중치를 찾는다. 
이는 반복적인 알고리즘으로 대규모 데이터셋에서도 효율적으로 가중치를 찾을 수 있다. 


### Cost Function
`Cost Function` 비용 함수는 예측기의 성능을 측정하는 함수이다. 
`ML` 모델의 성능을 평가하고 개선하는 방법은 데이터를 가장 잘 설명하는 파라미터를 찾는 것이다. 
이는 `Cost Function` 비용 함수 과 `Loss Function` 손실 함수를 사용할 수 있는데, 
이는 모델의 예측이 얼마나 `틀렸는지` 를 수치화하는 것이다. 
비용 함수는 하나의 선과 데이터 세트를 바탕으로 `cost` 비용 이라고 불리는 값을 반환한다. 
이 비용은 선이 데이터를 잘 근사하면 낮아지고, 선이 데이터를 잘 근사하지 못하면 높아진다. 
즉 비용이 낮을 수록 모델이 데이터를 잘 설명한다는 의미이다.  

가장 좋은 예측기는 비용 함수의 출력이 최소화하는 것을 의미하는데, 
아래 3가지 선인 `predication function` 예측 함수를 바탕으로 알아본다.  

![그림 1]({{site.baseurl}}/img/aiml/aiml-ml-basics-part1-8.drawio.png)


`붉은 예측기` 와 `노란 예측기` 는 데이터를 제대로 근사하지 못하는 것을 볼 수 있다. 
따라서 두 예측기의 비용함 수는 높은 비용이 높을 것이다. 
중간에 `파랑 예측기` 는 데이터를 매우 잘 근사하고 있으므로 낮은 비용을 가질 것이다.  

`Cost Function` 이 비용을 결과로 도출하는 방법은 매우 다양하지만, `mean squared error`(MSE) 평균 제곱 오차를 사용하는 것이 가장 일반적인 방법이다. 
여기서 오차(`error`) 는 데이터와 예측기 사이의 직선 거리 즉 차이($x_i - y_i$)를 의미한다. 
하나의 데이터 포이늩 $(x_i, y_i)$ 에서 $x_i$ 는 주택 면적, $y_i$ 는 주택 가격이며, 예측기가 `y(x)` 일떄 제곱 오차(`squared error`)는 아래와 같이 정의할 수 있다.  


$$
e = (y(x_i) - y_i)^2
$$

도출된 오차를 제곱해 모든 값이 양수가 될 수 있도록한다. 
그리고 모든 데이터 포인트에 대해 평균, 즉 `MSE` 를 구하기 위해 데이터를 모두 합산한다. 

$$
C = \frac{1}{N} \sum_{i=1}^N (x_i - y(x_i))^2
$$

위와 같이 모든 제곱 오차를 더한 다음 데이터 포인트의 수인 `N` 으로 나누게 되면 최종적으로 `MSE` 를 계산할 수 있다.  

### Gradient Descent
`Gradient Descent` 경사 하강법은 `Cost Function` 비용 함수를 최소화하는 가중치를 찾는 방법이다.
`Cost Function` 을 그래프로 나타내면(변수가 2개인 경우) 아래와 같이 표현할 수 있다.  

![그림 1]({{site.baseurl}}/img/aiml/aiml-ml-basics-part1-9.webp)


위 그림을 통해 비용 함수의 최솟값이 어느 부분에 있는지 눈으로 확인할 수 있다. 
하지만 실제 수백, 수천 혹은 그 이상의 파라미터를 사용하는 경우 이를 그래프로 표현하는 것은 불가능에 가깝다. 
그러므로 매우 고차원 비용 함수의 최소값을 찾기 위해 `Gradient Descent` 경사 하강법을 사용한다.  

경사 하강법의 동작 원리는 매우 단순하다. 만약 위 비용 함수 그래프 그림에서 공을 굴린다고 가정해 보자. 
그러면 공은 계속해서 표면 기울기가 가장 가파른 방향으로 굴러가고 결국 가장 아래쪽에서 멈추게 될 것이다. 
바로 이런 식으로 최소 비용을 찾는 것이 바로 경사 하강법의 방법이다. 
그래프에서 임의의 지점을 선택하고, 
가장 가파른 기울기의 방향을 찾고 그 방향으로 약간 움직이는 것을 반복해 비용 함수의 최솟값에 도달하게 되는 것이다. 
그리고 이렇게 찾아진 비용 함수의 파리미터가 바로 데이터의 분류하는 선을 그리기 위해 사용된다.  


### Conclusion
`ML` 의 요소 중 `Classification`, `Regression`, `Predictor`, `Cost Function`, `Gradient Descent` 에 대해 알아보았다.
이를 통해 기계 학습은 훈련 데이터를 통해 선을 그리는 것에 불과하다는 것을 수 있었다. 
선을 그리는 목적은 분류 알고리즘에서의 결정 경계 또는 현실 세계의 행동을 모델링하는 예측기를 결정하는 것이고, 
이러한 선은 경사 하강법을 통해 비용 함수의 최솟값을 찾아 생성할 수 있다는 것도 알아보았다. 
이렇게 `ML` 은 패턴 인식이라고 할 수 있다. 
`ML` 알고리즘은 훈련ㄷ ㅔ이터를 통해 선을 그려 패텬을 배우고, 이를 새로운 데이터에 일반화 하는 것을 의미한다.  



---  
## Reference
[Machine Learning Crash Course: Part 1 — Regression/Classification, Cost Functions, and Gradient Descent](https://medium.com/@ml.at.berkeley/machine-learning-crash-course-part-1-9377322b3042)  


